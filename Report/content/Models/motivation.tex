\section{Motivation}
The development of these models has been motivated by two disadvantages we have encountered while using the \gls{baum-welch}.
The first problem is that \gls{baum-welch} can be quite computationally expensive.

During this project, we have been using two different \gls{baum-welch} implementations. One written in Python, available at the PautomaC website, and another written in C\#, provided by the Accord.NET framework. The latter was definitely the fastest, it has thus been used in all of our tests.

For a 100-state \gls{hmm}, 10 iterations of \gls{baum-welch} on a small subset (5,000 random sequences) of the training set 1 from the PautomaC website took 510 seconds on an Intel Core i7-4500U. We have seen that converging with a threshold of 0.01 takes about 300 iterations on a training set of 5,000 sequences. Since the complexity of \gls{baum-welch} scales linearly with the number of sequences and iterations, training on a set of 100,000 sequences with 300 iterations would take about $306,000$ seconds, which is about $3.5$ days. And that is still a lower bound estimate considering that more iterations are typically required to converge when learning larger sets of data.

Since the \gls{baum-welch} considers all possible transitions from one state to another, we might be able to decrease its running time if we can lower the the number of transitions between states. As we argued in section \ref{sec:hmm-and-sparsity}, it seems like many problems that can be modeled with a \gls{hmm} call for a sparse transition matrix.
The second property of \gls{baum-welch} we think can be improved is the fact that a number of states to be used up front needs to be specified.
We think a better approach could be to use a small graph initially, then iteratively extend it based on some heuristics.

To summarize, the properties we are looking for in an algorithm is the ability to extend a small initial graph, but doing so in a way that keeps it sparse while still producing good results.