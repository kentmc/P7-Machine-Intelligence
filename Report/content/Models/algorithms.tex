\section{Algorithms}
In this section several algorithms are described, which all try to learn the parameters of a \gls{hmm} given a set of training sequences $D$, which contain a number of sequences over $m$ distinct symbols.
In other words, each algorithm will try to find a model that makes the generation of the particular validation sequences most likely.

The algorithms proposed in this section have been divided into two types.
The static algorithms start with a \gls{hmm} with a particular number of states which will never change.
The dynamic algorithms start with a small \gls{hmm} in regards to the number of states, and will dynamically extend the number of states through a number of iterations. 
For the static algorithms, $n$ denotes the number of states to be used.
For the dynamic algorithms, $n$ denotes a number of iterations $1, ..., n$, where each iteration extends the model by a single node.
In \ref{fig:alg-hierarchy} the names and classes of our proposed algorithms can be seen. Note that the \gls{baum-welch} has also been included, simply to emphasize which class of algorithms it belongs to. 

\begin{figure}[!h]
\Tree[.Algorithms
		[.{Static size} 
			{Baum-Welch}
            {Sparse Baum-Welch}
        ]
       	[.{Dynamic size} 
       		{Greedy Extend}
       		{State Splitting}
      	]
     ]
\caption{The algorithms used in our experiments}
\label{fig:alg-hierarchy}
\end{figure}

As the definition of the \gls{hmm} given in \ref{sec:hmm} is a little cumbersome to use in definition of some of the proposed algorithms, we introduce an alternate notation where the \gls{hmm} is seen as a graph $G$.

We further define the logarithmic likelihood of a sequence set $D$ as on a model $G$ as $LL_D(G)$.

Since the \gls{baum-welch} is guaranteed to never worsen $LL_D(G)$ when run on $G$ it is used internally by most of the algorithms.
By $BW_t(G, D)$ we denote the \gls{hmm} obtained after running \gls{baum-welch} on the \gls{hmm} $G$ using the training sequences $D$, and iterating as long as each iteration increases the logarithmic likelihood of the training sequences by at least a threshold $t\in \mathbb{R}$.
By $BW^i(G, D)$ we denote the \gls{hmm} obtained after running $i\in \mathbb{N}$ iterations of \gls{baum-welch} on the \gls{hmm} $G$ using the training sequences $D$.

\input{./content/Models/Algorithms/sparsebaumwelch.tex}
\input{./content/Models/Algorithms/greedyextend.tex}
\input{./content/Models/Algorithms/statesplitting.tex}
\input{./content/Models/Algorithms/variablengramextend.tex}
%\input{./content/Models/Algorithms/gammastatesplitter}
