\subsubsection{Greedy State Splitting}

This algorithm utilises a greedy concept to grow the hidden state space of a \gls{hmm} whilst maintaining a sparse transition matrix to preserve computability in large state spaces. The Greedy State Splitting relies heavily on the existing \gls{baum-welch} calling it repeatedly to achieve convergence.

Let $(n, s, \epsilon, D, V)$ be the input vector of the Greedy State Splitting algorithm where: $n, s \in \mathbb{N}, \epsilon\in[0,1], D$ is the training data set and $V$ is the validation data set. The greedy State Splitting algorithm starts with two vertex complete graph $G=K_2$ for the hidden state space with all the parameters randomised. Afterwards it iterates through the following phases until $|V(G)| = n$:

\begin{itemize}
	\item[] Phase 1, state splitting
	\item[1)] For each observable state sequence $O \in V$ a corresponding most probable hidden state sequence is computed using \gls{viterbi}: $Q=\mathcal{V}_G(O)$.
	\item[2)] For each vertex $v \in V(G)$ compute a score $s(v)$ as the probability the given vertex outputs the desired output symbol according to the precomputed hidden state sequences weighted by the probability of these sequences. In more formal terms, let $\xi_G(O=(o_1,...,o_T), v) = \{t\in\{1, ..., T\}|\mathcal{V}_G(O)_t=v\}$ then: $$s(v) = \sum_{O\in D}P(\mathcal{V}_G(O)|O,G) \frac{\sum_{t \in \xi_G(O, v)}b_v(o_t)}{|\xi_G(O, v)|}$$
	\item[3)] Find the ``weakest'' vertex: $$w = \argmin_{v\in V(G)}\{s(v)\}$$.
	\item[4)] Create a new graph $G' = G\cup \{w'\}$ where $w'$ is a new vertex such that: $\forall v\in V(G): a_{w'v} = a_{wv} \land a_{vw'} = a_{vw}$, $\forall \sigma \in \Sigma: b_{w'}(\sigma) = b_w(\sigma)$ and $\pi(w') = \pi(w)$.
	\item[5)] Normalise $G'$ so all the probabilities sum up to $1$.
	\item[] Phase 2, edge cutting
	\item[6)] For each edge $e = (v_1,v_2)\in E(G')$ check if the edge probability is lower then the given threshold: $a_{v-1,v_2}<\epsilon$. If so, remove the edge from the graph ($a_{v-1,v_2} = 0$).
	\item[] Phase 3, re-estimation of the model parameters.
	\item[7)] Run \gls{baum-welch} to re-learn the model parameters of the new model: $G = BW_t(G', D)$.
\end{itemize}

The algorithm has also been considered in a ``strict'' variation at first, where the edge cutting phase did not depend on the parameter $\epsilon$ but instead a constant out-degree was maintained for all vertices, namely the size of the output symbol alphabet $s = |\Sigma|$. The early results however showed, that the strict out-degree variation is outperformed by the $\epsilon$ threshold.