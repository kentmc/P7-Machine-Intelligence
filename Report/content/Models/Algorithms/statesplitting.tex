\subsection{State Splitting Approach}
\label{sec:state_splitting}
Another dynamic size \gls{hmm} learning approach was to construct a greedy heuristics to ``split'' states. Several versions and concepts were attempted whilst maintaining the \gls{baum-welch} as a basis for the approach, running it repeatedly on the growing model to ensure convergence. The notion of state splitting is not new and a highly specialised algorithm originally devised by Takami et al. has been successfully applied to phoneme recognition~\cite{takami1992, singer1996}. We on the other hand attempt to design a general state splitting algorithm with no dependency on the data structure.

The greedy state splitting algorithm consists of two main modules, with possible extensions. The main modules include the heuristics to identify the state (or multiple states) to split based on their usage inside the model and the splitting algorithm itself. During the experiments, two additional concepts, not fundamental for the state splitting itself, were also explored - edge cutting and state removal.

All the versions of the state splitting algorithm run a simple loop over several steps until a maximum number of states ($n$) is reached:
\begin{enumerate}
	\item Use a state identification heuristic to identify set of states $\mathcal{W}$ to split.
	\item Use a state splitting mechanic to split all states in $\mathcal{W}$.
	\item Run Baum-Welch for a given number of iterations (similar to $\beta$ value in the \acrlong{ge} algorithm, \ref{sec:greedy_extend}).
\end{enumerate}

It has been intended to replace the $n$ with a threshold mechanic $\theta$ to decrease the dependency on the prior knowledge of the size of the model, however the tests with a maximum number of states $n$ proved sufficient. Nevertheless employing a threshold mechanic can be an interesting extension in a future work, as such a few utilisations of a threshold are suggested in the approaches discussed below.

\subsubsection{State Split Mechanics}
Two main approaches to splitting the states were considered for the state splitting algorithm. One being very simple, just producing a copy of the state to split (\emph{Clone split}) and a much more elaborate approach considering the topology of the state inside the hidden state graph (\emph{Distribution split}).

\paragraph{Clone Split}
As aforementioned the clone split simply creates an exact copy of the state to split. Two variations have been proposed for copying the state in regards to handling input transition probabilities. Either splitting the probability of reaching the original state in half between the original state and the clone or simply copying the probability of the original node to the clone and normalising the probabilities through all transition afterwards.

No significant difference in performance has been observed between the two very similar variations. For clarity the latter one is described more formally by the following procedure:
\begin{enumerate}
	\item For each state to split $w \in\mathcal{W}$ construct a new state $w'$ such that $w'\notin S$.
	\item Set the initial, transition and emission probabilities as well as incoming transitions of the new state to copy the original:
	$$\forall w\in\mathcal{W}: \pi_{w'} = \pi_w \wedge \forall s\in S: (a_{sw'} = a_{sw} \wedge a_{w's} = a_{ws}) \wedge \forall \sigma\in\Sigma: b_{w'}(\sigma)=b_w(\sigma)$$
	\item Normalise the initial probabilities:
	$$\forall s\in S: \pi_s = \frac{\pi_s}{\sum_{r\in S}\pi_r}$$
	\item Normalise the transition probabilities:
	$$\forall s,r\in S: a_{sr} = \frac{a_{sr}}{\sum_{p\in S}a_{sp}}$$
\end{enumerate}

The state $w'$ created by the above procedure will have exactly the same parameters as the original $w$. This in itself has proven to be a huge disadvantage of the \emph{Clone split} methods as the \gls{baum-welch} was unable to distinguish between the two states and thus learn them to have different parameters. A possible solution would be to randomise part or all of the parameters of the newly created state $w'$, however this approach resembles the \hyperref[sec:greedy_extend]{Greedy Extend} very much and was thus not attempted.

The above described problem proved fatal for the \emph{Clone split} approach despite the attempts to alleviate it by incorporating other concepts such as edge cutting and state removal.

\paragraph{Distribution split}
The \emph{Distribution split} mechanic chooses to split the state based on how uniform the transition and emission probability distributions of the given state are. As such, the method in itself incorporates two thresholds $\epsilon_1$ for the transition probability distribution and $\epsilon_2$ for the emission probability distribution that determines whether the states determined to be split $w\in\mathcal{W}$ are suitable to be split or not.

The idea behind the distribution split was based around the fact that a highly uniform distribution of the outgoing transitions or emissions shows that the node has a high usage value as determined by the used state identification heuristic, due to performing several different tasks in the model. To split the workload on the node the new node was created to substitute for half of the responsibilities of the original node. As such half of the out-transition probabilities of the original node were set to zero, whilst their original values would be moved as probabilities of out-transitions of the new state, whilst preserving the rest at zero values. Similarly, if the emission probabilities had more uniform distribution, the emission probabilities would split in half between the original and new states (in the case of the emissions, the probabilities were not set to zero, but rather to a very low non-zero value to allow the \gls{baum-welch} to relearn those probabilities if necessary). As such a sparse transitions matrix was derived directly from the data.

A different behaviour than expected was observed for the \emph{Distribution split}, however, as a case of ``over-splitting'' has occurred forcing most of the states to have just a single non-zero probability out-transition looping back to themselves.

A randomised version of the mechanic was later devised to avoid the exceedingly aggressive splitting. The new mechanic assigns all in- and out-transition probabilities of the new state to a random value and only preserves the emission probabilities and initial probability that are created as an exact copy of the original state.

In order to shield the algorithm against getting stuck in a local optima, the state will be split regardless of the uniformity of the distribution if the algorithm has gone 10 iterations without adding a new state.

The exact mechanic of the split including the threshold mechanic is described by the following procedure:

$\forall w\in\mathcal{W}:$
\begin{enumerate}
	\item Determine if the algorithm has been stuck for more than ten iterations in the stagnation indicator $\omega$:
	$$ \omega = \begin{cases}
true &\text{if algorithm has been stuck for 10 iterations} \\
false &\text{otherwise}
\end{cases} $$
\item Determine if the transition probability distribution of $w$ is uniform as sum of differences from average:
\begin{align*}
Z_w &= \{a_{wJ} \in \mathbf{A} \vert a_{wJ} \neq 0\} \\
\overline{z_w} &= \frac{\sum_{a_{wJ} \in z_w} a_{wJ}}{\vert z_w \vert} \\
AbsE_{w,1} &= \sum_{a_{wJ} \in z_w} \vert a_{wJ} - \overline{z_w} \vert \\
\end{align*}
\item Determine if the emission probability distribution of $w$ is uniform as sum of differences from average:
\begin{align*}
Z_w &= \{b_{w}(\sigma) \in \mathbf{B} \vert b_{w}(\sigma) \neq 0 \wedge \sigma \in \Sigma\} \ \\
\overline{z_w} &= \frac{\sum_{b_{w}(\sigma) \in z_w} b_{w}(\sigma)}{\vert z_w \vert} \\
AbsE_{w,2} &= \sum_{b_{w}(\sigma) \in z_w} \vert b_{w}(\sigma) - \overline{z_w} \vert \\
\end{align*}
\item Split $w$ if either the transition or the emission probability distribution is uniform or the algorithm is stuck, creating a new state $w'$ such that $w'\notin S$.
\begin{align*}
\bar{S} &= \begin{cases}
S \cup \{ w' \} &\text{if } (AbsE_{w,1} < \epsilon_1) \vee (AbsE_{w,2} < \epsilon_2) \vee \omega\\
S &\text{otherwise} \\
\end{cases} \\
\mathbf{\bar{A}} &= \begin{cases}
[(\mathbf{A} [a_{w's_1},  ... , a_{w's_n} \in [0,1]])[a_{s_1w'},  ... , a_{s_nw'}, a_{w'w'} \in [0, 1]]^T] &\text{if } (AbsE_1 < \epsilon_1)\\&\text{}\vee (AbsE_2 < \epsilon_2)\\&\text{}\vee \omega\\
\mathbf{A} &\text{otherwise} \\
\end{cases}\\
\mathbf{\bar{B}} &= \begin{cases}
[\mathbf{B} \text{ } [b_{w'}(\sigma_1),  ... , b_{w'}(\sigma_m)]]
\text{ where } b_{w'}(\sigma) = b_{w}(\sigma)
&\text{if } (AbsE_1 < \epsilon_1) \\
&\text{ }\vee (AbsE_2 < \epsilon_2) \vee \omega\\
\mathbf{B} &\text{otherwise} \\
\end{cases}\\
\overline{\boldsymbol{\pi}} &=\begin{cases}
[\boldsymbol{\pi}[\pi_w]]&\text{if } (AbsE_1 < \epsilon_1) \vee (AbsE_2 < \epsilon_2) \vee \omega\\
\boldsymbol{\pi} &\text{otherwise} \\
\end{cases}
\end{align*}
\item Normalise the \gls{hmm} $\overline{\lambda} = (\mathbf{\overline{A}}, \mathbf{\overline{B}}, \overline{\boldsymbol{\pi}})$.
\item Set $\lambda = \overline{\lambda}$
\end{enumerate}

\subsubsection{State Identification Heuristics}
Similarly to state splitting itself, two main approaches were explored for identifying the best state to split. The first one utilised the \gls{viterbi} thus named the \emph{Viterbi heuristic}, whilst the second one uses the $\gamma_t(i)$ variables computed during the \gls{baum-welch} and was therefore named the \emph{Gamma heuristic}.

Both of the heuristics compute a score $\zeta:S \rightarrow \mathbb{R}$ for each hidden state of the model, that can later be utilised to determine the state to split.

\paragraph{Viterbi Heuristic}
The \emph{Viterbi heuristic} computes the score $\zeta$ for each hidden state as a probability of producing the correct symbol in the training sequences for which it belongs to the corresponding most probable hidden state sequence as determined by the \gls{viterbi}.

In more formal terms, the computation of the score $\zeta(s)$ for each $s \in S$ and a given \gls{hmm} $\lambda = (\mathbf{A}, \mathbf{B}, \boldsymbol{\pi})$ can be described by the following procedure:
\begin{enumerate}
	\item For each signal $\mathbf{O}\in D$ a corresponding most probable hidden state sequence is computed using the \gls{viterbi}: $\mathbf{Q}=\mathcal{V}_G(\mathbf{O})$.
	\item For each state $s\in S$ determine the significant positions in the hidden state sequences given by \gls{viterbi}:
	$$\forall s\in S,\forall \mathbf{O}=(o_1, ..., o_T)\in D: \tau_{s, \lambda}^\mathbf{O}=\{t\in\{1, ..., T\}|\mathbf{Q}_t=s\}$$
	\item For each state $s \in S$  and $\mathbf{O}\in D$ compute the partial score (performance) $\hat\zeta_{\mathbf{O}}(s)$ as the average probability of producing the expected observable symbol in accordance to the signal $\mathbf{O}$ over all the significant positions for the given state $s$ and signal $\mathbf{O}$:
	$$\forall s\in S,\forall \mathbf{O}\in D: \hat{\zeta}_{\mathbf{O}}(s) = \frac{\sum_{t\in\tau_{s, \lambda}^{\mathbf{O}}}b_s(o_t)}{|\tau_{s, \lambda}^{\mathbf{O}}|}$$
	\item Finally, compute the $\zeta$ score for each state $s\in S$ as a sum of all the partial scores for the given state weighted by the probabilities of the associated signals being generated by the corresponding hidden state sequences (as compute by \gls{viterbi}):
	$$\forall s\in S: \zeta(s)=\sum_{\mathbf{O}\in D}P(\mathbf{Q}|\mathbf{O}, \lambda)\hat\zeta_{s, \lambda}^{\mathbf{O}}$$
\end{enumerate}

The obtained $\zeta$ scores the states of the \gls{hmm} based on their ``performance'' for the tasks they are most likely to perform. As such the state with the lowest score is determined to be the worst performing state $w$: $$w = \argmin_{s\in S}(\zeta(s))$$

This node is deemed to be the worst performing one as a result of being involved in the generation of many of the signals in the validation set $V$. As such, it seems meaningful to split the node into two, in order to share the extensive workload and increase performance.

The \emph{Viterbi heuristic} can be straightforwardly extended to identify more than just one state to split, thereby producing a set of the worst performing nodes $\mathcal{W}$. The set can be constructed iteratively starting with $\mathcal{W} = \emptyset$ as:

$$\mathcal{W} = \mathcal{W} \cup \{\argmin_{s\in S\setminus \mathcal{W}}(\zeta(s))\}$$
until $|\mathcal{W}|$ equals the desired number of states to split.

A further modification of the \emph{Viterbi heuristic} was considered to incorporate the use of a splitting threshold $\theta$ instead of a maximum number of states. For this purpose a normalised version of the score $\overline{\zeta}$ was introduced:
$$\overline{\zeta}(s) = \frac{\zeta(s)}{\sum_{s\in S}\zeta(s)}$$

The states to split $\mathcal{W}$ were thus determined as all states that scored below the given threshold $\theta$:
$$\mathcal{W} = \{s\in S|\overline\zeta(s) < \theta\}$$

Improvements to the \emph{Viterbi heuristic} were considered, mainly including the \emph{n-step Viterbi heuristic} that would have computed the score on not only the output probability in the given significant position, but also on the probability of correctly outputting the next $n -1$ symbols of the given signal - starting from the explored state - to further increase precision. The above described version of the \emph{Viterbi heuristic} would be considered \emph{1-step Viterbi heuristic} in this context. This approach however remains untested due to preference of the \emph{Gamma heuristic} and can be considered for future work.

\paragraph{Gamma Heuristic}

The \emph{Gamma heuristic} assigns higher $\zeta$ values to the states that are visited the most when generating the 
training sequences. This is done by calculating and comparing $\gamma$ variables from the \gls{baum-welch}, for each of the states in the model. As of now the score is simply computed as a sum of all the $\gamma$ variables through all signal $\mathbf{O}\in D$ and along their whole length:
$$\forall i\in\{1,...,n\}: \zeta{S_i} = \sum_{\mathbf{O}\in D}\sum_{t=1}^{T}\gamma_t(S_i)$$

The obtained $\zeta$ scores the states of the \gls{hmm} based on their ``overuse''. As such the largest score is used to determine the worst performing state $w$:
$$w = \argmax_{s\in S}(\zeta(s))$$

It is important to note the difference here compared to the \emph{Viterby heuristic} which assigns the smallest score to the worst performing nodes.

In the case of \emph{Gamma heuristic} the $\zeta$ measure is pretty straightforward, as it directly identifies the states that are used the most during generation of the training sequences $D$ as such being very susceptible to be overloaded.

Analogically to the \emph{Viterbi heuristic} the \emph{Gamma heuristic} can also be extended to account for a set of worst performing states $\mathcal{W}$ instead of just a one state. Starting with $\mathcal{W}=\emptyset$:
$$\mathcal{W} = \mathcal{W} \cup \{\argmax_{s\in S\setminus \mathcal{W}}(\zeta(s))\}$$
until $|\mathcal{W}|$ equals the desired number of states to split.

Again a normalisation is possible to include threshold mechanics very similar to that of the \emph{Viterbi heuristic}. As the \emph{Gamma heuristic} was run alongside the \emph{Distribution split} mechanic which incorporates a threshold approach that is believed to better reflect the actual data the threshold implementation inside the \emph{Gamma heuristic} itself was not attempted an can be considered for future work.

\subsubsection{Edge Cutting}
In attempt to induce a data derived sparsity into the model while using the \emph{Clone split} an approach of cutting unsaturated edges from the model has been devised. The concept of cutting in this context means setting the transition probability to $0$.

Two different methods of cutting edges had been tested. The first one labeled as ``strict'' kept a constant out (in) degrees for the nodes and a different edge cutting approach that was forcing transition probability to zero once it dropped below a certain threshold.

\paragraph {Strict Edge Cutting}
The strict edge cutting method was motivated by lowering the computational complexity of the \gls{baum-welch} algorithm. As the \gls{baum-welch} runs with the complexity $\mathcal{O}(Tn^2+Tnm)$ where $n$ is the number of states $m$ is the number of symbols and $T$ is the length of the training sequence, we attempted to lower the complexity to simply $\mathcal{O}(Tnm)$. As such an upper bound of $m$ has been placed on the out degree of all the states: $$\forall s\in S: d^-(s) \le m$$ where the $d^-(s)$ is the out degree of the node $s$ in the sense of number of non-zero out-transitions of the node $s$.

An analogical approach is possible with using the in degrees $d^+(s)$.

The edge cutting occurred immediately after splitting of states introduced a breach of the above property. The edge cutting for every state $s\in S$ repeats the following procedure short procedure until $d^-(s) \le m$ holds:
\begin{enumerate}
	\item Identify the edge to cut as the least probable outgoing transition:
	$$r = \argmin_{r\in S}(a_{sr})$$
	\item Remove the edge by setting the probability to zero: $$a_{sr} = 0$$
	\item Normalise remaining transitions:
	$$\forall r \in S: a_{sr} = \frac{a_{sr}}{\sum_{p\in S}a_{sp}}$$
\end{enumerate}

The above method of edge cutting had the property of guaranteeing the maximum number of transitions in a model, however the deterministic density of the transition matrix imposed huge limitations on the ability to derive the hidden state space topology from data. A similar approach is utilised in the Greedy Extend (\ref{sec:greedy_extend}) however the constriction imposed by a fixed number of non-zero transitions introduced is alleviated by running the \gls{baum-welch} to observe if an improvement was actually achieved and trying a different random configuration if not.

\paragraph {Threshold Edge Cutting}
Contrary to the strict edge cutting, using threshold does not only remove edges once there's too many outgoing (incoming) non-zero transition from one node but introduces a more flexible metric. The threshold mechanic introduces a threshold $\epsilon$ that determines how significant must a transition be if the model is to consider it. In more formal terms:
$$\forall s,r\in S: a_{sr} < \epsilon \Rightarrow a_{sr} = 0$$

During our experiments the threshold edge cutting has been executed after each state splitting iteration. 

The experimental results have shown that edge cutting using either the strict or the threshold versions of the algorithm did not produce desired results. Cutting edges often forced the learning process to diverge and often made the algorithm run for prolonged amounts of time contrary to the initial intention of decreasing complexity. It is highly possible that the failure of the edge cutting approach was due to the \emph{Clone split} that was employed alongside the edge cutting algorithm. Another explanation is use of unsuitable edge cutting approaches as a far more elaborate algorithm based on learning a sparse \gls{hmm} has been introduced by Bicego et al.~\cite{bicego2007}~achieving success on the used data.

\subsubsection{State Removal}
The state removal was initially introduced only as a cleanup algorithm to remove unreachable states or ``dead ends'' with no successor. An extended version was later employed to be used with the \emph{Viterbi heurisitc} for determining states to split and the \emph{Clone split} mechanics. The extended version was used to remove all states that were not utilised in any of the hidden state sequences given by \gls{viterbi} for the signals in the training data. More formally (using notation from \emph{Viterbi heuristic} definition) the set of removed states $\mathcal{R}$ is defined as:
$$\mathcal{R}=\{s\in S|\forall\mathbf{O}\in D: |\tau_{s,\lambda}^\mathbf{O}|=0\}$$

The extended state removal algorithm however proved to be quite aggressive in the context of the \emph{Clone split} as almost all new states were removed in the next iteration after being added.

\subsubsection{The Final State Splitting Algorithm}
The final version of the state splitting approach used for the experiments utilised a combination of a stochastic version of \emph{Distribution split} and the \emph{Gamma heuristic}.

As both original state split mechanics have experienced severe problems during our experiments, the final version of the state splitting algorithm incorporates a a hybrid randomised state splitting mechanic based on the \emph{Distribution split}. The method is considerably similar to the \acrlong{ge} (\ref{sec:greedy_extend}) is created, however a dense transition matrix is maintained.

The \emph{Gamma heuristic} was employed due to the intention to utilise the splitting thresholds associated with the \emph{Distribution split}. The threshold mechanic of the \emph{Viterbi heuristic} is deemed to reflect the data worse and thus be less suitable for data derived learning of the structure. This is mainly because of the heavy reliance of the \emph{Viterbi heuristic} threshold on the number of stats itself and thus the setting of the threshold has a lot of influence over the final number of states.

As there was no benefit observed for multiple splitting multiple states per iteration, as such only one state was split during each iteration ($|\mathcal{W}| = 1$).

As the \emph{Gamma heuristic} is utilised the algorithm is further referred to as \acrfull{gs}.

%This algorithm utilises a greedy concept to grow the hidden state space of a \gls{hmm} whilst maintaining a sparse transition matrix to preserve computability in large state spaces. The Greedy %State Splitting relies heavily on the existing \gls{baum-welch} calling it repeatedly to achieve convergence.

%Let $(n, s, \epsilon, D, V)$ be the input vector of the Greedy State Splitting algorithm where: $n, s \in \mathbb{N}, \epsilon\in[0,1], D$ is the training data set and $V$ is the validation data %set. The greedy State Splitting algorithm starts with two vertex complete graph $G=K_2$ for the hidden state space with all the parameters randomised. Afterwards it iterates through the %following phases until $|V(G)| = n$:

%\begin{itemize}
%	\item[] Phase 1, state splitting
%	\item[1)] For each observable state sequence $\mathbf{O} \in V$ a corresponding most probable hidden state sequence is computed using \gls{viterbi}: %$\mathbf{Q}=\mathcal{V}_G(\mathbf{O})$.
%	\item[2)] For each vertex $v \in V(G)$ compute a score $s(v)$ as the probability the given vertex outputs the desired output symbol according to the precomputed hidden state %sequences weighted by the probability of these sequences. In more formal terms, let $\theta_G(\mathbf{O}=(o_1,...,o_T), v) = \{t\in\{1, ..., T\}|\mathbf{Q}_t=v\}$ then: $$s(v) = %\sum_{\mathbf{O}\in V}P(\mathbf{Q}|\mathbf{O},G) \frac{\sum_{t \in \theta_G(\mathbf{O}, v)}b_v(o_t)}{|\theta_G(\mathbf{O}, v)|}$$
%	\item[3)] Find the ``weakest'' vertex: $$w = \argmin_{v\in V(G)}\{s(v)\}$$.
%	\item[4)] Create a new graph $G' = G\cup \{w'\}$ where $w'$ is a new vertex such that: $\forall v\in V(G): a_{w'v} = a_{wv} \land a_{vw'} = a_{vw}$, $\forall \sigma \in \Sigma: %b_{w'}(\sigma) = b_w(\sigma)$ and $\pi(w') = \pi(w)$.
%	\item[5)] Normalise $G'$ so all the probabilities sum up to $1$.
%	\item[] Phase 2, edge cutting
%	\item[6)] For each edge $e = (v_1,v_2)\in E(G')$ check if the edge probability is lower then the given threshold: $a_{v-1,v_2}<\epsilon$. If so, remove the edge from the graph %($a_{v-1,v_2} = 0$).
%	\item[] Phase 3, re-estimation of the model parameters.
%	\item[7)] Run \gls{baum-welch} to re-learn the model parameters of the new model: $G = BW_t(G', D)$.
%\end{itemize}

%The algorithm has also been considered in a ``strict'' variation at first, where the edge cutting phase did not depend on the parameter $\epsilon$ but instead a constant out-degree was %maintained for all vertices, namely the size of the output symbol alphabet $s = |\Sigma|$. The early results however showed, that the strict out-degree variation is outperformed by the %$\epsilon$ threshold.