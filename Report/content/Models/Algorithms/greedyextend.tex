\subsection{Greedy Extend}
\label{sec:greedy_extend}
Initially, a graph representation $G$ of a single state \gls{hmm} is created. The single node has the initial probability set to 1, loops to itself with probability 1, and its emission probabilities for each of the $m$ symbols are chosen randomly and normalised.

The following pseudo code describes how the algorithm continuously tries to extend the graph, as long as it improves the likelihood of the data:
\begin{enumerate}
\item Repeat until convergence:
	\item $G'$ = $(V(G) \cup \{y'\}, E(G))$, where $y'$ is a new node with a random initial probability in range $[0, 1]$ having random emission probabilities for all $s$ symbols, which sums to $1$.
	\item Randomly choose a set of nodes $Y = \{y_1, y_2, ... , y_l\}$ from $V(G')$, where $l = \lceil \log |V(G')| \rceil$ and $\forall a,b: y_a \neq y_b$.
	\item For each $y \in Y$, the transitions $(y, y')$ and $(y', y)$ are added to $E(G')$ with random transition probabilities.
	\item Normalize $G'$.
	\item If $LL_D(BW^{\beta}(G', D)) > LL_D(G)$, let $G = LL_D(BW^{\beta}(G', D))$.
\end{enumerate}

\paragraph{Measuring convergence}
Convergence can be measured based on the increase in log likelihood of the training data, in the same way as \gls{bw} uses a threshold of convergence.

\paragraph{Determining the $\beta$ Value}
Another big question about the \gls{ge} algorithm, is how the choice of $\beta$ affects the performance of the algorithm.
As $\beta$ denotes the number of \gls{baum-welch} iterations to run each time the algorithm attempts to extend the graph, increasing $\beta$ will also increase the run time of the algorithm. It may be the case that better results are achieved when $\beta$ is increased, since more iterations of \gls{baum-welch} also means a greater increase in likelihood. However, it could be the case that using many iterations early increases the chance of getting trapped in a local optimum.
An experiment has been conducted of using different values for $\beta$ on data set $1$ from the PautomaC competition. The results can be seen in figure \ref{fig:ge-different-thresholds-tested}. Each line represents the mean value of 5 runs of the \gls{ge} algorithm with the specified number of iterations. Some of the plots have been cut off at a point where one of the runs did not manage to extend beyond a certain number of states.

\begin{figure}[!h]
\begin{centering}
\begin{tikzpicture}
	\pgfplotsset{every axis legend/.append style={ 
		at={(0.5,1.06)},
		anchor=south}}
	\begin{axis}[
			scale = 1.5,
			xlabel = Number of states,
            	ylabel = Log likelihood,
            	legend columns=-1,
            	legend entries={IT-0, IT-1, IT-2, IT-3, IT-5, IT-10, IT-50},
			legend style={/tikz/every even column/.append style={column sep=0.3cm}}]
		
		\addplot+[mark=none]table[x=States, y=IT-0, col sep=tab]
		{content/Experiments/graphdata/ge-intermediate-iterations-test.csv};
		
		\addplot+[mark=none]table[x=States, y=IT-1, col sep=tab]
		{content/Experiments/graphdata/ge-intermediate-iterations-test.csv};
		
		\addplot+[mark=none]table[x=States, y=IT-2, col sep=tab]
		{content/Experiments/graphdata/ge-intermediate-iterations-test.csv};
		
		\addplot+[mark=none]table[x=States, y=IT-3, col sep=tab]
		{content/Experiments/graphdata/ge-intermediate-iterations-test.csv};

		\addplot+[mark=none]table[x=States, y=IT-5, col sep=tab]
		{content/Experiments/graphdata/ge-intermediate-iterations-test.csv};
		
		\addplot+[mark=none]table[x=States, y=IT-10, col sep=tab]
		{content/Experiments/graphdata/ge-intermediate-iterations-test.csv};
		
		\addplot+[mark=none]table[x=States, y=IT-50, col sep=tab]
		{content/Experiments/graphdata/ge-intermediate-iterations-test.csv};
	\end{axis}
\end{tikzpicture} 
\caption{Test of different values for $\beta$ while running the \gls{ge} algorithm.}
\label{fig:ge-different-thresholds-tested} 
\end{centering}
\end{figure}

The figure shows surprisingly that using no iterations is somehow better than using just a single iteration. However, using 5 or more iterations is better than not using any iterations at all.
Further experiments with $\beta = 5$, since it looks like the performance does not change significantly when increasing the number of iterations beyond 5.
