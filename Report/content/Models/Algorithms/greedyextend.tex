\subsection{Greedy Extend}
\label{sec:greedy_extend}
Initially, a graph representation $G$ of a single state \gls{hmm} is created. The single node has the initial probability set to 1, loops to itself with probability 1, and its emission probabilities for each of the $m$ symbols are chosen randomly and normalised.

The following pseudo code describes how the algorithm continuously tries to extend the graph, as long as it improves the likelihood of the training data $D$:
\begin{enumerate}
\item Repeat until convergence:
	\item $G'$ = $(V(G) \cup \{y'\}, E(G))$, where $y'$ is a new node with a random initial probability, a random probability of transitioning to itself, and random emission probabilities for all $m$ symbols, normalised so they sum to $1$.
	\item Randomly choose a set of nodes $X'$ from $V(G')$, where $|A| = \lceil \log |V(G')| \rceil$.
	\item For each node $x \in X$, the transitions $(x, y')$ and $(y', x)$ are added to $E(G')$ with random transition probabilities.
	\item Normalize $G'$.
	\item If $LL(BW^{\beta}(G')) > LL(G)$, let $G = LL(BW^{\beta}(G'))$.
\end{enumerate}
where $BW^{\beta}(G')$ denotes the \gls{hmm} obtained by running \gls{bw} for $\beta$ iterations on $G$.

\paragraph{Measuring convergence}
Convergence can be measured based on the increase in log likelihood of the training data, in the same way as \gls{bw} uses a threshold of convergence.