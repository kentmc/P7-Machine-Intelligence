\subsection{Variable length n-gram Extend}
\label{sec:vl-ngram-extend}
This algorithm consists of two main modules. The first one creates a rough initial graph $G$, the second one extends it. The extend algorithm used in the experiments was the Greedy Extend as described in \ref{sec:greedy_extend}.

A symbol sequence may be produced by several state sequences. Therefore, the intuition behind the first module is to create a rough initial graph $G$ which disregards this possibility. Once created, the extending algorithm gradually smoothes this rough approximation.

\subsubsection{Graph creation}

The graph $G$ is created following a variable length $n$-gram approach. A typical $n$-gram approach is to learn and then detect subsequences of length $n$. Just like $n$-grams, this variable length n-gram splits the sequence into subsequences. Our algorithm splits every sequence $\mathbf{O}\in D$ according to the following process.

In the graph $G$, there is a single initial state, with an initial probability of 1. All the other states have an initial probability of 0. This is because the initialisation module simplifies the setting, considering there is only one initial state. The extend module can then increase the number of possible initial states, normalising the initial probabilities.

The graph is created with that single initial state. The initialisation module then runs through the same state-adding process for every sequence $\mathbf{O}$ in the training set.

\paragraph{State-adding process}

It looks for the set $S$ of states that can produce the highest number of successive symbols found in the sequence $\mathbf{O}$, starting from the first symbol in $\mathbf{O}$.

If such a $S$ is found, it means this subsequence $\mathbf{O_i}$ of $\mathbf{O}$ can already be produced by the current graph $G$, hence it is split from the rest of the sequence $\mathbf{O}$. 

In the case it fails to find any $S$, in order to complete $G$, a new state is created which can produce the second symbol - sequences of length 1 are always produced by the single initial state. The algorithm then has a $S$, consisting of this state and the previous one, that can produce a subsequence $\mathbf{O_i}$.

The process is then repeated from the next symbol right after $\mathbf{O_i}$, until the whole sequence is split.

As such, instead of having a fixed length $n$ for those subsequences, our variable length n-gram fetches the set $S$ of states able to produce the largest subsequence $\mathbf{O_i}$ from the current symbol in $\mathbf{O}$. Once the sequence has been completely split into subsequences, each last state of a subsequence is made to be able to transition to the first state of the next subsequence. In pseudo-code:

\begin{itemize}
	\item Repeat until the end of the sequence $\mathbf{O}$:
	\begin{itemize}
		\item If a set of states can produce any subsequence from the beginning of $\mathbf{O}$, the set $S$ producing the largest subsequence $\mathbf{O_i}$ is kept => $\mathbf{O}$ = $\mathbf{O_i} + \mathbf{O'}$, where $\mathbf{O'}$ is the rest of the sequence $\mathbf{O}$.
		\item If no set $S$ of states can produce any such subsequence => creates a new state producing the second symbol, $S$ becomes that state and the previous one, and can now produce a subsequence $\mathbf{O_i}$.
		\item increments i and sets $\mathbf{O}$ to now be $\mathbf{O'}$
	\end{itemize}
	\item For each $\mathbf{O_i} \in \mathbf{O} = \{\mathbf{O_1}, \mathbf{O_2}, ... , \mathbf{O_l-1}\}$, with $l$ the length of the sequence $\mathbf{O}$, a transition is added between the last state of $\mathbf{O_i}$ and the first state of $\mathbf{O_i+1}$, with random transition probabilities.
	\item Normalise $G$.
\end{itemize}

Experiments have shown this can sometimes lead to several states having very few transitions. Keeping the same intuition in mind, those states are all merged into one if their number of transitions is less than $\alpha$.

\paragraph{Determining the $\alpha$ Value}

The question about the proper value for $\alpha$ to be used then comes up. Thus, different values have been used on the same data set to evaluate the performance of each for this algorithm.

As $\alpha$ denotes the minimum initial number of transitions each state can have, higher values will reduce the initial state space but likely reduce the accuracy of the model, while lower values would likely keep the accuracy high but might provide very large models.

The following experiments have been conducted by using different values for $\alpha$ on data sets $1$ and $23$ from the \emph{PAutomaC} competition. The results can be seen in figures \ref{fig:vlnge-different-init-trans-tested-1} and \ref{fig:vlnge-different-init-trans-tested-23}. Each line represents the score obtained depending on the number of states, with the specified minimum initial number of transitions.


\begin{figure}[!h]
\begin{centering}
\begin{tikzpicture}
	\pgfplotsset{every axis legend/.append style={ 
		at={(0.5,1.06)},
		anchor=south}}
	\begin{axis}[
			scale = 1.2,
			xlabel = Number of states,
            	ylabel = Log likelihood,
            	legend columns=-1,
            	legend entries={IMnbT-1, IMnbT-3, IMnbT-5, IMnbT-10},
			legend style={/tikz/every even column/.append style={column sep=0.3cm}}]
		
		\addplot+[mark=none]table[x=States, y=IMnbT-1, col sep=comma]
		{content/Experiments/graphdata/vlnge-different-init-trans-tested-1.csv};
		
		\addplot+[mark=none]table[x=States, y=IMnbT-3, col sep=comma]
		{content/Experiments/graphdata/vlnge-different-init-trans-tested-1.csv};
		
		\addplot+[mark=none]table[x=States, y=IMnbT-5, col sep=comma]
		{content/Experiments/graphdata/vlnge-different-init-trans-tested-1.csv};
		
		\addplot+[mark=none]table[x=States, y=IMnbT-10, col sep=comma]
		{content/Experiments/graphdata/vlnge-different-init-trans-tested-1.csv};
	\end{axis}
\end{tikzpicture} 
\caption{Test of different values for $\alpha$ while running the Variable length n-gram Extend algorithm on data set 1.}
\label{fig:vlnge-different-init-trans-tested-1} 
\end{centering}
\end{figure}


\begin{figure}[!h]
\begin{centering}
\begin{tikzpicture}
	\pgfplotsset{every axis legend/.append style={ 
		at={(0.5,1.06)},
		anchor=south}}
	\begin{axis}[
			scale = 1.2,
			xlabel = Number of states,
            	ylabel = Log likelihood,
            	legend columns=-1,
            	legend entries={IMnbT-1, IMnbT-3, IMnbT-5, IMnbT-10},
			legend style={/tikz/every even column/.append style={column sep=0.3cm}}]
		
		\addplot+[mark=none]table[x=States, y=IMnbT-1, col sep=tab]
		{content/Experiments/graphdata/vlnge-different-init-trans-tested-23.csv};
		
		\addplot+[mark=none]table[x=States, y=IMnbT-3, col sep=tab]
		{content/Experiments/graphdata/vlnge-different-init-trans-tested-23.csv};
		
		\addplot+[mark=none]table[x=States, y=IMnbT-5, col sep=tab]
		{content/Experiments/graphdata/vlnge-different-init-trans-tested-23.csv};
		
		\addplot+[mark=none]table[x=States, y=IMnbT-10, col sep=tab]
		{content/Experiments/graphdata/vlnge-different-init-trans-tested-23.csv};
	\end{axis}
\end{tikzpicture} 
\caption{Test of different values for $\alpha$ while running the Variable length n-gram Extend algorithm on data set 23.}
\label{fig:vlnge-different-init-trans-tested-23} 
\end{centering}
\end{figure}

As can be observed on those figures, increasing the minimum initial number of transitions severely decreases the precision of the model. On the other hand, not merging any state at all may sometimes provide a larger, less accurate model - but this is not necessarily the case. The best value for a safe baseline appears to be 3, in the happy middle.