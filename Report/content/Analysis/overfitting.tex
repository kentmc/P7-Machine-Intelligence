\subsection{The Overfitting Problem}
When a model is trained and evaluated from data as it is delivered by pautomac, there is a chance of overfitting a model. Overfitting can happen if the set of observations in the training data is not equal to the set of observations in the evaluation data, or in other words if the evaluation data is unseen. The reason for this is simply that if the training data is different from the evaluation data, and the model is perfectly fitted to the training data alone, the unseen evaluation data will fall in probability.

A simple solution to the overfitting problem is to use validation data. Validation data is a secondary training set, that is only used for computing the temporary likelihood for the model while training. Technically the test data from pautomac could be used for validation, but doing so could cause the final evaluation to be less representative of the model's accuracy. The solution that is used for this work, is to split the training set so that at least $\frac{1}{3}$ of the training data is validation data, and at the same time at most $\frac{2}{3}$ of the training data is used for learning a model.