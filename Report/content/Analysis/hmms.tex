\section{Hidden Markov Models}

Hidden Markov models are one of the most widely used and best known variants of probabilistic finite automata~\cite{pautomacTR, Rabiner89hmm}. The theoretical basis for hidden Markov models and associated methods and algorithms were first described in a series of works by L. E. Baum~et~al.~\cite{baum1966, baum1967, baum1968, baum1970, baum1972}.

One can view a hidden Markov model as an extension of a standard Markov chain. A Markov chain based model has the property that every observed symbol corresponds directly to an associated state of the model. This property is too restrictive for numerous problems. Hidden Markov models therefore introduce the concept of hidden (unobservable) states with a probability distribution over the observable symbols. The generated symbol thus becomes a probabilistic function of the current hidden state. This extension allows application of hidden Markov models to a much larger variety of problems, however also poses new complications, such as increased complexity of evaluation of probability of a given signal (sequence of observable symbols), determining the optimal sequence of hidden states for a given signal or estimation of parameters for the model~\cite{Rabiner89hmm}.

To better illustrate the mechanics behind the hidden Markov model we provide a simple example. Consider a lottery game with several urns filled with balls of different colours. Every urn may hold different number of balls of a certain colour, or may not even contain balls of some colours at all. In every turn an unbiased arbiter selects one urn randomly (but possibly abiding to some rules given by the urn selected previously) and takes out a random ball out of the selected urn. You are then shown the colour of the ball, not however, from which urn it was taken. The ball is then returned to the appropriate urn and the game continues so forth your goal being to predict the colours that come next. In this simple case, you can observe a sequence of symbols - colours generated by what can be viewed as a hidden Markov model, where the urns are the hidden states and the observable symbol probability distribution for every urn (state) is given by the colours of the balls inside.

More formally, a finite discrete hidden Markov model with $n \in \mathbb{N}$ (hidden) states $S = \{S_1, S_2, ..., S_n\}$ over an alphabet of $m \in \mathbb{N}$ observable symbols $\Sigma=\{\sigma_1, \sigma_2, ..., \sigma_m\}$ is a tuple: $$\lambda = (A, B, \pi)$$
Where:
\begin{itemize}
	\item[$A$] is an $n$ times $n$ square matrix such that an element of the matrix ${a_{ij} \in [0, 1]}$ represents the transition probability from state $S_i$ to state $S_j$. Naturally this implies ${\forall i \in \{1, 2, ..., n\}: \sum_{j=1}^n{a_{ij}} = 1}$.
	\item[$B$] is an $n$ times $m$ matrix such that an element of the matrix ${b_{ij} \in [0, 1]}$ represents the probability of outputting symbol $\sigma_j$ in state $S_i$. Naturally this implies ${\forall i \in \{1, 2, ..., n\}: \sum_{j=1}^m{b_{ij}} = 1}$.
	\item[$\pi$] is a vector of $n$ variables, ${\pi=(\pi_1, ..., \pi_n) \in [0, 1]^n}$. Where the value $\pi_i$ represents the probability of state $S_i$ being the initial state. Naturally this implies ${\sum_{i=1}^n{\pi_i} = 1}$
\end{itemize}

Such a hidden Markov model can be used to generate a sequence of observable symbols (signal): $$O = (o_1, ..., o_T)$$
Where ${\forall t \in \{1, ..., T\}: o_t \in \Sigma}$, $o_t$ is the symbol observed at time $t$ and $T$ is the number of discrete time steps during the observation (number of observed symbols).

Similarly, we denote a sequence of hidden states of the hidden Markov model as: $$Q = (q_1, ..., q_T)$$
Where ${\forall t \in \{1, ..., T\}: q_t \in S}$, $q_t$ is the state of the model at time $t$ and $T$ is the number of visited states.
We define the probability of the hidden state sequence (walk) $Q$ given the model $\lambda$ as:
$$P(Q|\lambda) = \pi_{q_1}a_{q_1q_2}...a_{q_{T-1}q_T}$$

Any signal $O$ generated by the hidden Markov model has a corresponding sequence of hidden states the model visited while generating $Q$ of the same length. This sequence is typically unknown for modeled real world signals and generally numerous different hidden state sequences may generate the observed signal with different probabilities. We denote the probability of model $\lambda$ producing the observation sequence $O$ by hidden state sequence $Q$ as:
$$P(O|Q,\lambda) = b_{q_1}(o_1)b_{q_2}(o_2)b_{q_T}(o_T)$$

In the above probability definitions we use mode lenient notation for simplicity. As the use of this notation is preserved throughout the following sections alongside the notation from definition of hidden Markov model we define it formally for clarity:
\begin{itemize}
	\item[] $a_{S_iS_j}$ where $S_i, S_j\in S$ is used to refer to the value of $a_{ij}$.
	\item[] $\pi_{S_i}$ where $S_i\in S$ is used to refer to the value of $\pi_i$.
	\item[] $b_{S_i}(\sigma_j)$ where $S_i\in S$ and $\sigma_j\in \Sigma$ is used to refer to the value of $b_{ij}$.
\end{itemize}

Furthermore let us define the set of all possible walks through hidden state space of model $\lambda$ of length $T$ as $\mathcal{Q}_\lambda^T$. Thus the the probability of model $\lambda$ generating the signal $O$ can be easily defined as a sum of probabilities of generating the given signal through all hidden state walks weighted by probabilities of the particular hidden state walks: 

\begin{align*}
P(O|\lambda)&=\sum_{Q\in\mathcal{Q}^T_\lambda}{P(O|Q,\lambda)P(Q|\lambda)}\\
&=\sum_{Q\in\mathcal{Q}^T_\lambda}{\pi_{q_1}b_{q_1}(o_1)a_{q_1q_2}b_{q_2}(o_2)...a_{q_{T-1}q_T}b_{q_T}(o_T)}
\end{align*}

It should be noted that the hidden Markov model defined here is finite in the sense of both $n$ and $m$ being finite numbers. An extension to model allowing infinite number of states, symbols or both is rather straightforward, however we will not be needing this extension for the purposes of this publication and thus it will be omitted.

\subsection{Hidden Markov Model Evaluation}
It is meaningful and desirable to evaluate a model once constructed and learned. Such evaluation is usually done by computing the probability of the model producing a certain observation sequence. Therefore for a model $\lambda = (A, B, \pi)$ and a (test) sequence $O$, we want to learn $P(O|\lambda)$. Going by the above definition one arrives at a summation over all possible walks through hidden state space of model $\lambda$ of length $T$. It is simple to see that, with the presumption that the model is unrestricted (i.e., matrix $A$ is dense), there is exponentially many of such walks in terms of their length $T$ ($|\mathcal{Q}_\lambda^T|\in\mathcal{O}(N^T)$). Computing the given summation and thereafter computing $P(O|\lambda)$ by definition is computationally infeasible.

Luckily, an iterative dynamic programming approach exists that can help us compute the coveted probability, called the \emph{Forward-Backward Procedure}~\cite{baum1967, baum1968}. The \emph{Forward-Backward Procedure} is composed of computing two separate sets of variables (forward and backward), both of which can be used to compute the probability $P(O|\lambda)$.

The forward variable $\alpha_t(i)$, formally defined as: $$\alpha_t(i)=P((o_1, ..., o_t), q_t=S_i|\lambda)$$ describes the probability that we observe the first $t$ symbols of the given signal and end in the state $S_i$ at the time $t$ given the model $\lambda$.

The forward variable can be computed iteratively as:
\begin{align*}
\forall i\in \{1, ..., n\}&: \alpha_1(i)=\pi_ib_{S_i}(o_1)\\
\forall i\in \{1, ..., n\}, t\in\{2, ..., T\}&: \alpha_t(i) = \sum_{j=1}^n{(\alpha_{t-1}(j)a_{ij})}b_{S_i}(o_t)
\end{align*}

It is easy to see that the probability of an observable sequence $O$ can be obtained by summing through the forward variables at time $T$ for all of the hidden states:
\begin{align*}
P(O|\lambda) &= \sum_{i=1}^n{P(O, q_T=S_i|\lambda)}\\
&= \sum_{i=1}^n{\alpha_T(i)}
\end{align*}

Analogically, the backward variable $\beta_t(i)$, formally defined as: $$\beta_t(i)=P((o_t, ..., o_T)| q_t=S_i, \lambda)$$ describes the probability that we observe the last $(T-t+1)$ symbols of signal $O$ given we start at state $S_i$ at the time $t$ and the model $\lambda$.

Again, computing the backward variable is possible iteratively:
\begin{align*}
\forall i\in \{1, ..., n\}&:\beta_T(i)=b_{S_i}(o_T)\\
\forall i\in\{1, ..., n\}, t\in\{1, ..., T-1\}&:\beta_t(i)=\sum_{j=1}^n{(\beta_{t+1}(j)a_{ij})}b_{S_i}(o_t)
\end{align*}

Once again we straightforwardly obtain a simple way of computing $P(O|\lambda)$:
\begin{align*}
P(O|\lambda) &= \sum_{i=1}^n{P(O|q_1=S_i, \lambda)}\\
&= \sum_{i=1}^n{\beta_1(i)}
\end{align*}

After a careful observation one can see that both approaches, using forward or backward variables, give as an efficient way to compute $P(O|\lambda)$ in complexity $\mathcal{O}(Tn^2)$~\cite{Rabiner89hmm}.

\subsection{Optimal Hidden State Sequence}

It may be useful to determine the hidden state sequence the model used to generate a given observable signal. Which hidden state sequence is optimal is heavily dependent on the optimality criterion. Multiple optimality criteria exist and are meaningful to use for some applications. The most widely used optimality criterion maximises the probability of the whole walk through the hidden state space given the model $\lambda$ and the observable sequence $O$~\cite{Rabiner89hmm}:
$$\max_{Q\in\mathcal{Q}_T^\lambda}(P(Q|O, \lambda))$$

An algorithmic solution exists for optimisation of maximisation of $P(O, Q|\lambda)$, equivalent to the above expression, based on dynamic programing methods, called \emph{Viterbi Algorithm}~\cite{Viterbi1967, Forney1973}. The algorithm iteratively computes the best single state path score for the first $t$ symbols of the observed sequence. We denote this score for state $i$ and time $t$ as $\theta_t(i)$:
$$\theta_t(i) = \max_{q_1, ..., q_{t-1}}(P((o_1, ..., o_t), (q_1, ..., q_{t-1}), q_t=i|\lambda))$$

The \emph{Viterbi Algorithm} is briefly illustrated by the following pseudocode:
\begin{lstlisting}[mathescape=true]
real, int[] Viterbi (int n, int T, real[,] A,
 real[,] B, real[] $\pi$, int[] O) begin
   real[,] $\theta$ := new real[T,n] //$\theta$[t,i] = $\theta_t(i)$
   int[,] $\psi$ := new int[T,n] //For backtracking
   real p := 0.0 //Best score
   int[] q := new int[T] //Optimal sequence
   
   //Initialisation
   for int i := 1 to n do begin
      $\theta$[1,i] := $\pi$[i]B[i,O[1]]
      $\psi$[1,i] := 0
   end
   
   //Iteration
   for int t := 2 to T do begin
      for int j := 1 to n do begin
         $\theta$[t,j] := 0
         for int i := 1 to n do begin //Maximum over i
            if $\theta$[t,j] < $\theta$[(t - 1),i]A[i,j]B[j,O[t]] then
             do begin
               $\theta$[t,j] := $\theta$[(t - 1),i]A[i,j]B[j,O[t]]
               $\psi$[t,j] := i
            end
         end
      end
   end
   
   //Termination
   for int i := 1 to n do begin //Maximum over i
      if p < $\theta$[T,i] then do begin
         p := $\theta$[T,i]
         q[T] := i
      end
   end
   
   //Backtracking
   for int t := (T - 1) downto 1 do
      q[t] := $\psi$[(t + 1), q[t + 1]]

   return p, q;
end
\end{lstlisting}

\subsection{Estimating Model Parameters}

There is no analytical solution known for finding model parameters $A$, $B$, $\pi$ maximising the probability of a given observable sequence. In fact, no optimal way to estimate the parameters for hidden Markov models exists. However, an iterative method, the \emph{Baum-Welch} algorithm~\cite{baum1970}, can be used to derive $\lambda = (A, B, \pi)$ such that $P(O|\lambda)$ is locally maximised for a given signal $O$. The \emph{Baum-Welch} algorithm has been shown to be equivalent to the \emph{expectation maximisation} (EM) method for hidden Markov models~\cite{Dempster1977, wu1983}.

In this section we present a simple iterative approach to estimation of hidden Markov model parameters based primarily on the \emph{Baum-Welch} algorithm. This approach works for a single observable symbol sequence, however the \emph{Baum-Welch} algorithm can be extended to account for train data composed of multitude of signals~\cite{levinson1983, li2000}.



\subsection{Classification}

In general case, the hidden Markov model as defined would be ergodic - between each couple of states, there exists a finite path with non-zero probability. For many applications it is desirable to restrict the model in some fashion. The most commonly known of such restrictions is the so called left-right model also known as the Bakis model. This model only allows transitions in the hidden state space in one direction, formally: $$\forall i,j \in \{1, ..., n\}; i < j: a_{ji} = 0$$
The left-right model is used extensively for speech recognition.~\cite{bakis1976, jelinek1976}.

The hidden Markov model considered here follows the standard definition where the observable symbol is outputted in a state of the model. A modification is possible and used, again in the field of speech recognition, in which the outputted observable symbol is associated with a transition instead of a state~\cite{Rabiner89hmm, jelinek1983}. It has been proven useful to incorporate transitions that output no symbol (null transitions) into this modification~\cite{jelinek1983}.

The hidden Markov model defined in this section is a discrete hidden Markov model, with both discrete probability distribution of the observable symbols as well as a discrete distribution of transition probabilities on hidden states. It is possible to generalise a hidden Markov model by changing both of the afore mentioned probability distributions into continuous ones. Typically the Gaussian distribution is used~\cite{cappe2005, piyathilaka2013}. In general case however, exact inference is infeasible in hidden Markov models with continuous latent variables and approximation methods must be used (extended Kalman filter, particle filter)~\cite{cappe2005}.

As was mentioned before, the definition we presented is for a finite model but can be extended to account for infinite number of states or observable symbols straightforwardly. Many other variants, modifications and extensions also exist, some of them hinted in~\cite{Rabiner89hmm}.