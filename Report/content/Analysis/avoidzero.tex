\subsection{Avoiding Zero Values}

When training a model it is not certain that all possible sequences of symbols are read. Because of this uncertainty there is a chance that some sequences is not given a probability in the model, in other words a zero probability represents that sequence. Also if no measures are taken against it, underflows might happen for very small probabilities. In either cases a zero value will be present in the resulting model. Avoiding these zero values is important, as the zero probability will otherwise be propagated through multiplication, where a single transformation in a sequence with no probability, will mean that the entire sequence has no probability. In the case of PautomaC we evaluate a test by using the normalized probability of 1000 sequences in the perplexity equation. If just one of these sequences has a zero probability the entire test will evaluate to zero probability, even if the other 999 sequences has good probabilities. The reason for this is that the logarithm for a zero valued probability evaluates to minus infinite, which will of course negate the probability of all the other sequences.


One method for avoiding zero values is smoothing. With smoothing the probabilistic weight from the more probable sequences are moved slightly to the less probable ones. This ensures that zero weights will be non existing. Underflows can also be avoided using smoothing, it is however depended on the smoothing technique.


In the case of the PautomaC competition, the test data is available together with the training data. This means that if the models are trained on both the training and test data, smoothing can effectively be avoided, as all possible sequences will be read and given some probability. Underflow is however still an issue, where a negative logarithm representation can be used to ensure that even extremely small probabilities do not underflow.