%\section{description}
The goal for our experiments is to discover strength and weaknesses for our algorithms and models, given data created by different systems. As described in section \ref{sec:pautomac} Pautomac used 3 parameters for their machines: the state space, the transition density and the amount of symbols used in the model. Naturally these parameters produce learning complexity in different ways, where we hope to illustrate what parameter effects each algorithm and model, as well as observe an overall logarithmic likelihood score. In an attempt to produce concise results, we have chosen to only use a subset of 11 datasets from Pautomac, how these datasets were chosen is described in section \ref{sec:datasets}. In section \ref{sec:parameters} we have studied what parameters the experiments should by run with, including the amount of training data used, \gls{bw}'s threshold and number of \gls{bw} iterations for the \gls{ge} and \gls{gs} algorithm. 

The approach used for each parameter experiment have been to cover a range of different state space sizes, going from 10 to 100 states. \gls{bw} of course had to be probed on different sate amounts, where our dynamic algorithms have been build to reach either some local maxima or stop at 100 states. The \gls{sbw} algorithm has been used next to \gls{bw} to give a comparison baseline for training sparse models.

Finally in section \ref{sec:results} the results for each parameter experiment can be found. A couple concrete Pautomac competition "submission attempt" has been added as well, to show how our work relates to some of the best results from the competition.