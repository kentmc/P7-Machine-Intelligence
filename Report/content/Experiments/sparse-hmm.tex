\subsubsection{Sparse Hidden Markov Model}
\label{sec:'shmm}

As many of implemented experimental algorithms work with \glspl{hmm} that have sparse transition matrix in the sense of many transition probabilities being zero, we deemed it useful to optimise the algorithms for sparse matrix environment. For this purpose a new model, the \emph{Sparse hidden Markov model}, was implemented to utilise the sparseness of the matrix to achieve increased performance speed compared to standard methods.

The \emph{Sparse hidden Markov model} optimisation is done mainly in the form of storing information on active (non-zero probability) transitions in the model. To achieve this every node remembers all active successors and all active predecessors. This allows the algorithms to only read and compute the values relevant to the underlying \gls{hmm}, thus achieving computational speedup without loss of precision.

The above optimisation build on the property of the \gls{baum-welch} that the probability of a transition will stay as either zero or non-zero (unless an underflow occurs) during iterative update of the parameters.

The above information is utilised by both \gls{baum-welch} and \gls{fb_algorithm}, which is required for evaluation of the model and in the \gls{baum-welch} itself. The \gls{viterbi} was also optimised using the above knowledge. Exhaustive testing has been done to ensure correctness of the algorithm as well as measure the obtained speedup. The tests were conducted using randomly generated dummy data and randomly initialised models. Eight different runs were conducted for every configuration to filter out possible noise. For each run a different random initial configuration was generated and used for both the standard \gls{hmm} and the optimised sparse version. For tests of the \gls{baum-welch} $20$ iterations were conducted for each run for both the standard and sparse versions. The average speedup achieved on either the \gls{baum-welch} or \gls{viterbi} can be seen in graphs \ref{sparseBWspeedup} and \ref{sparseViterbispeedup} respectively.

\begin{figure}
	\centering
	\begin{tikzpicture}
		\begin{axis}[
			width=0.8\textwidth,
			height=0.32\textheight,
			ymin=1,
			ymax=5,
			xlabel = Number of states,
            		ylabel = Average speedup,
            		legend style={at={(0,1)}, anchor=north west}]
			\addplot+table[x=States, y=BW_8, col sep=tab]
			{content/Experiments/graphdata/sparseHMMspeedup10.csv};
			\addlegendentry{A}
			\addplot+table[x=States, y=BW_2, col sep=tab]
			{content/Experiments/graphdata/sparseHMMspeedup10.csv};
			\addlegendentry{B}
			\addplot+table[x=States, y=BW_2, col sep=tab]
			{content/Experiments/graphdata/sparseHMMspeedup5.csv};
			\addlegendentry{C}
		\end{axis}
	\end{tikzpicture}
	\caption{Average speedup achieved on \gls{baum-welch} using the \emph{Sparse hidden Markov model}. \textbf{A:} Tested a model with transition density of 10\% on dummy data with 8 symbols. \textbf{B:} Tested a model with transition density of 10\% on dummy data with 2 symbols. \textbf{C:} Tested a model with transition density of 5\% on dummy data with 2 symbols.}
	\label{sparseBWspeedup}
\end{figure}

\begin{figure}
	\centering
	\begin{tikzpicture}
		\begin{axis}[
			width=0.8\textwidth,
			height=0.32\textheight,
			ymin=10,
			ymax=40,
			xlabel = Number of states,
            		ylabel = Average speedup,
            		legend style={at={(0,1)}, anchor=north west}]
			\addplot+table[x=States, y=VIT_8, col sep=tab]
			{content/Experiments/graphdata/sparseHMMspeedup10.csv};
			\addlegendentry{A}
			\addplot+table[x=States, y=VIT_2, col sep=tab]
			{content/Experiments/graphdata/sparseHMMspeedup10.csv};
			\addlegendentry{B}
			\addplot+table[x=States, y=VIT_2, col sep=tab]
			{content/Experiments/graphdata/sparseHMMspeedup5.csv};
			\addlegendentry{C}
		\end{axis}
	\end{tikzpicture}
	\caption{Average speedup achieved on \gls{viterbi} using the \emph{Sparse hidden Markov model}. \textbf{A:} Tested a model with transition density of 10\% on dummy signal with 8 symbols of length 10000. \textbf{B:} Tested a model with transition density of 10\% on dummy signal with 2 symbols of length 10000. \textbf{C:} Tested a model with transition density of 5\% on dummy signal with 2 symbols of length 10000.}
	\label{sparseViterbispeedup}
\end{figure}

All of the performance tests were coded in C\# programming language just as the rest of the test environment and were solely single threaded. The performance speedup was measured running on a 4-core Intel Core i7-4700MQ processor machine clocked at frequency 2.4 GHz with 8 GB of available \gls{ram} and utilising the Microsoft Windows 8 operating system.

The result graph \ref{sparseBWspeedup} shows that the \gls{baum-welch} speedup using the \emph{Sparse Hidden Markov Model} is largely independent of the number of symbols in the data, however strongly correlated to the negative density (or ``sparsity'') of the model of the transition matrix. The speedup measured for the \gls{baum-welch} achieved a factor of $2.53$ for the 10\%  data density and a factor of $4.72$ for the 5\% density for a $100$ state model. The observed speedup is lower than the expected theoretical value (factor of 10 for 10\% density and 20 for 5\% density). This may be caused by unideal implementation, use of more and more complex data structures and possibly optimisation performed by the compiler.

A significant speedup has been observed for the \gls{viterbi}. As can be observed from the result graph, \ref{sparseViterbispeedup}, the speedup on the \gls{viterbi} is very similar to speedup on \gls{baum-welch}, being independent on the number of symbols and almost linear to sparsity of the model. A speedup by a factor of $20.99$ has been observed for the model with 10\% density, but this factor was increased to $38.01$ with 5\% density model. Both of the discussed results are the highest measured speedups observed for $100$ state models. Contrary to \gls{baum-welch} results, \gls{viterbi} scored better than the expected theoretical speedup.