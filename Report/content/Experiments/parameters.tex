\subsection{Experiment Parameters}\label{sec:parameters}
The learners introduced require a number of different input parameters. It is likely that different value for the parameters will affect the result. In the following, a number of experiments are conducted, aiming to find suitable parameters for all of the learners. 

\subsubsection{Static learners}
Since the static learners, namely \gls{bw} and \gls{sbw}, use a \gls{hmm} internally, the number of states of that \gls{hmm} is required as input.
The Pautomac models that generated the data sets were generated by at most 73 states, and due to the fact, that the computation effort increases quadratically with the number of states, we deem it unnecessary to test models with more than 100 states. Due to the large amount of time it takes to run the \gls{baum-welch}, a step size of 10 for the number of states tested.
The dynamic learners does not require a specified number of states, as they all start with a model containing just a single state, which in turns are extended.

The data sets given by Pautomac ranges in sequences. Some have 20.000 sequences while others have 100.000. These amounts of sequences are however larger than what is necessary to illustrate how one algorithm behaves, given some dataset. Another problem is that the larger datasets takes significantly longer to compute, compared to a subset of sequences. It is therefore interesting to explore what amount of sequences that will both produces useful results, without spending an unnecessary amount of time doing the computations.

\paragraph{Training Sequences}
The Pautomac training sets contain between 20,000 and 100,000 sequences.
An experiment has been conducted, to see whether all of these sequences are needed to learn a good model using \gls{baum-welch}.
The result is depicted in Figure \ref{fig:sequences}. It seems like it does not improve the result significantly when using more than 100 sequences.
However, it was decided to use 5000 sequences as the running time of \gls{baum-welch} for this amount is still reasonable. It should be noted that the running time increases linearly with the amount of sequences used for training, but quadratically by the number of states, following the complexity of the \gls{fb_algorithm} algorithm.

The setup for this experiment:
\begin{itemize}
\item Dataset: 36
\item Algorithm: Baum Welch
\item Threshold: 0.01
\item Training sequences: 100-50,000
\item Validation sequences: 1,000
\item CPU: 2 Ghz Intel core 2 duo
\end{itemize}

\begin{figure}
	\centering
	\begin{subfigure}[b]{0.5\textwidth}
        \begin{tikzpicture}
		\begin{axis}[
				scale = 0.9,
				ymin = 23000,
				ymax = 24000,
				ybar,
				xtick=data,
   				symbolic x coords={100,500,1000,5000,10000,50000},
				xlabel = Sequences,
            		ylabel = Score (lower is better)]
				\addplot+table[y=score, col sep=tab]
				{content/Experiments/graphdata/sequences.csv};
		\end{axis}%
	\end{tikzpicture}
        \end{subfigure}%
		\begin{subfigure}[b]{0.5\textwidth}
\begin{tikzpicture}
		\begin{axis}[
				scale = 0.9,
				ymin = 0,
				ybar,
				xtick=data,
   				symbolic x coords={100,500,1000,5000,10000,50000},
				xlabel = Sequences,
            		ylabel = Running time (seconds)]
				\addplot+table[y=time, col sep=tab]
				{content/Experiments/graphdata/sequences.csv};
		\end{axis}%
	\end{tikzpicture}
	\end{subfigure}
  	\caption{Plot of Baum-welch score and runningtime on different sequence amounts}\label{fig:sequences}
\end{figure}


\paragraph{Baum-Welch Treshold}
The Baum-Welch algorithm takes two parameters, the amount of states which the trained model should consist of and the threshold of convergence. The state range for the experiments have already been selected to range between 10 and 100 with a step size of 10. Unlike the choice of states, which is a variable, the threshold should be a static value for all the experiments.

Defining the threshold is done by exploring how Baum-Welch behaves on different threshold values. \ref{fig:threshold} shows the results from the experiments. Each threshold parameter value were combined with the parameter of 50 states, and trained on a single dataset.

\begin{figure}
\centering
	\begin{tikzpicture}
		\begin{axis}[
				ybar,
				xtick=data,
   				symbolic x coords={0.1,0.05,0.01,0.005,0.001,0.0001},
				xlabel = Treshold,
            		ylabel = Log likelihood of the validation data]
				\addplot+table[y=Score, col sep=tab]
				{content/Experiments/graphdata/treshold.csv};
		\end{axis}
	\end{tikzpicture}
\caption{Negative logarithmic likelihood when running Baum-Welch with different thresholds.}
\label{fig:threshold}
\end{figure}

The results show that a threshold below 0.01 does not improve the likelihood of the validation data significantly, thus a threshold of 0.01 will be used for future tests.

\subsubsection{Greedy Extend}

\todo{Fix this section}
The setup for this experiment:
\begin{itemize}
\item Dataset: 36
\item Algorithm: Baum Welch
\item Threshold: 0.01
\item Training sequences: 5,000
\item Validation sequences: 1,000
\end{itemize}

10 BW iterations between when after each extend.
100 attempts to extend graph.
0.01 threshold for BW when reaching a maximum of 100 states or if not able to expand further.

Whether we choose to run Greedy Extend a single or multiple times does not seem to have a big impact on the result.
In figure \ref{fig:ge-diversity-5-runs}, the result of 5 different runs of Greedy Extend can be seen. The results of all runs look very similar.
\begin{figure}
\begin{centering}
\begin{tikzpicture}
	\pgfplotsset{every axis legend/.append style={ 
		at={(0.5,1.03)},
		anchor=south}}
	\begin{axis}[
			xlabel = Number of states,
            	ylabel = Score (lower is better),
            	legend columns=-1,
            	legend entries={Run 1, Run 2, Run 3, Run 4, Run 5},
			legend style={/tikz/every even column/.append style={column sep=0.3cm}}]
		
		\addplot+[mark=none]table[x=States, y=Run1, col sep=tab]
		{content/Experiments/graphdata/ge-diversity-5-runs.csv};
		
		\addplot+[mark=none]table[x=States, y=Run2, col sep=tab]
		{content/Experiments/graphdata/ge-diversity-5-runs.csv};
		
		\addplot+[mark=none]table[x=States, y=Run3, col sep=tab]
		{content/Experiments/graphdata/ge-diversity-5-runs.csv};
		
		\addplot+[mark=none]table[x=States, y=Run4, col sep=tab]
		{content/Experiments/graphdata/ge-diversity-5-runs.csv};

		\addplot+[mark=none]table[x=States, y=Run5, col sep=tab]
		{content/Experiments/graphdata/ge-diversity-5-runs.csv};
	\end{axis}
\end{tikzpicture} 
\caption{5 different runs of Greedy Extend on the same data set using $\alpha = 100, \beta = 10$.}
\label{fig:ge-diversity-5-runs} 
\end{centering}
\end{figure}
