\chapter{Future Work}
\label{chap:future_work}
The report introduces several approaches to dynamically learning the structure of an \gls{hmm}. Despite several variations being made to the explored algorithms and extensive testing performed more research is needed to confirm the exact quality and usability of the proposed approaches in general setting as well as attempt further improvements in the approaches proposed.

Several areas explored in the report could benefit from further experiments. Deeper exploration for instance, may be necessary to fully unravel and explain the multitude of different behaviours the algorithms displayed when being tested on the PautomaC data sets. Running the algorithms on larger models to obtain more results as well as testing on more of the PautomaC datasets could be considered to be able to confirm the stated hypotheses or reveal new findings about the nature of the algorithms on a specific data. Another option to refine the results is to conduct more runs of the same algorithms with the same configuration and on the same datasets. These could greatly help to reduce possible noise in the results induced by the random initialisation of the models and/or the impact of the stochasticity in the learning process itself (as inclusion of new states in both \acrlong{ge} and \acrlong{gs} is done at least partially randomly).

More experiments could also serve to confirm the results obtained when testing for the experiment parameters (\ref{sec:parameters}). Most of the tests were only conducted on a single dataset and a single run. This could possibly bring in a lot of noise into the results and confirming the derived results with different datasets might be beneficial as large differences in the behaviour of the algorithms were observed between individual datasets. Similarly as with the other experiments more runs of the same algorithms and settings could be conducted to reduce the possible noise in the results.

Even the algorithm speed experiment (\ref{sec:speedrun}) deserves more attention than we were able to expend during the report work. The result showed relatively unexpected results with the \gls{ge} Learner improving continuously. Some hypotheses were formulated to justify the huge score gap the \gls{ge} created however more extensive testing would be required to confirm those hypotheses or on the other hand prove them flawed. The continued improvement of the \gls{ge} learner also suggest running even longer experiments as well as experiments of similar type on different datasets as varied behaviour of the \gls{ge} Learner has been observed while testing on multiple different datasets \ref{sec:dataset_experiments}. Once again the reduction of noise may also play an important role and more runs are thus suggested. More runs may be particularly useful in regards to the running speed comparison between \acrlong{bw} and \acrlong{sbw} Learners.

Among the experimental results presented, the one most conceivable to require further work is the comparison to the actual PautomaC scores. The scores obtained during our tests were considerably lacking compared to the PautomaC competitors. As mentioned this is attributed to a slightly different paradigm regarding the length of the generated sequences we have employed compared to the PautomaC data generating models. A retest with the algorithms modified as suggested in section \ref{sec:hmm_vs_pa} might provide a much more relevant results.

On final note, both tested dynamic algorithms (\acrlong{ge} and \acrlong{gs}) relied heavily on adding a completely or at least partially randomised states to an existing and partially learned \gls{hmm}. This is heavily influenced by the failure of the deterministic state inclusion algorithms (section \ref{sec:state_splitting}) to produce sensible results. The area however seems to need more exploration as no specific reason exists why a stochastic state inclusion is necessary. More research may therefore reveal a condition that poses the stochasticity requirement or, on the other hand aid in a development of a deterministic method for adding states into a semi-learned \gls{hmm}.

