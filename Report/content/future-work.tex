\chapter{Future Work}
\label{chap:future_work}
The report introduces several approaches to dynamically learning the structure of an \gls{hmm}. Despite several variations being made to the explored algorithms and extensive testing performed more research is needed to confirm the exact quality and usability of the proposed approaches in general setting as well as attempt further improvements in the approaches proposed.

Several areas explored in the report could benefit from further experiments. Deeper exploration for instance, may be necessary to fully unravel and explain the multitude of different behaviours the algorithms displayed when being tested on the PautomaC data sets. Running the algorithms on larger models to obtain more results as well as testing on more of the PautomaC datasets could be considered to be able to confirm the stated hypotheses or reveal new findings about the nature of the algorithms on a specific data. Another option to refine the results is to conduct more runs of the same algorithms with the same configuration and on the same datasets. These could greatly help to reduce possible noise in the results induced by the random initialisation of the models and/or the impact of the stochasticity in the learning process itself (as inclusion of new states in both \acrlong{ge} and \acrlong{gs} is done at least partially randomly).
Most naturally, the experiment results can be utilised to further tweak and refine the parameters of the algorithms themselves to achieve better performance.

More experiments could also serve to confirm the results obtained when testing for the experiment parameters (\ref{sec:parameters}). Most of the tests were only conducted on a single dataset and a single run. This could possibly bring in a lot of noise into the results and confirming the derived results with different datasets might be beneficial as large differences in the behaviour of the algorithms were observed between individual datasets. Similarly as with the other experiments more runs of the same algorithms and settings could be conducted to reduce the possible noise in the results.

Even the algorithm speed experiment (\ref{sec:speedrun}) deserves more attention than we were able to expend during the report work. The result showed relatively unexpected results with the \gls{ge} Learner improving continuously. Some hypotheses were formulated to justify the huge score gap the \gls{ge} created however more extensive testing would be required to confirm those hypotheses or on the other hand prove them flawed. The continued improvement of the \gls{ge} learner also suggest running even longer experiments as well as experiments of similar type on different datasets as varied behaviour of the \gls{ge} Learner has been observed while testing on multiple different datasets \ref{sec:dataset_experiments}. Once again the reduction of noise may also play an important role and more runs are thus suggested. More runs may be particularly useful in regards to the running speed comparison between \acrlong{bw} and \acrlong{sbw} Learners.

Among the experimental results presented, the one most conceivable to require further work is the comparison to the actual PautomaC scores. The scores obtained during our tests were considerably lacking compared to the PautomaC competitors. As mentioned this is attributed to a slightly different paradigm regarding the length of the generated sequences we have employed compared to the PautomaC data generating models. A retest with the algorithms modified as suggested in section \ref{sec:hmm_vs_pa} might provide a much more relevant results.

Taking a different perspective the computational performance of the algorithms may be further addressed. A very interesting point would be exploration of the sparseness property of the \gls{hmm} transition matrix and a possibility for further optimisation of the \gls{hmm} related algorithms. A better optimisation is deemed highly possible as the speedup achieved for the presented sparse optimisation are not up to par with the theoretical expectation.

Another implementation related improvement could be an improved scaling algorithm or other anti-underflow measure. We have included scaling algorithm in our \gls{baum-welch} implementation, however as has been observed during several of the experiments the scaling was not sufficient to prevent underflow. As such improving on this aspect could allow the experiments to finish with sensible data.

Apart from the discussed dynamic algorithms many different approaches can be considered to derive the structure of an \gls{hmm} from the data. An approach opposite to the state splitting, called the state pruning has been presented by Bicego et al.~\cite{bicego2003}. And among other approaches a static pre-computation of the \gls{hmm} can be considered for further exploration.

In regards to the proposed algorithms for dynamic learning both tested algorithms (\acrlong{ge} and \acrlong{gs}) relied heavily on adding a completely or at least partially randomised states to an existing and partially learned \gls{hmm}. This is heavily influenced by the failure of the deterministic state inclusion algorithms (section \ref{sec:state_splitting}) to produce sensible results. The area however seems to need more exploration as no specific reason exists why a stochastic state inclusion is necessary. More research may therefore reveal a condition that poses the stochasticity requirement or, on the other hand aid in a development of a new deterministic method for adding states into a semi-learned \gls{hmm}.

We believe it is also worthwhile exploring the heuristics of the state splitting approach. Both the presented approaches \emph{Viterbi heuristic} and \emph{Gamma heuristic} are deemed to identify states to split sensibly in regards to the properties of the \gls{hmm}. Several improvements and extensions have also been proposed for the algorithms and utilising a splitting threshold alongside the proposed heuristics shows promise of replacing the need for a fixed number of states at the beginning for a threshold for which a value may exist optimal for large amounts of diverse data. A further work in this area may therefore reveal a learning method for \glspl{hmm} completely independent on a user input for number of states and instead being able to derive the optimal number of states from the data itself.

Another interesting update to the algorithms and tests alike could be more elaborate way of using the training data in order to prevent overfitting. As of now, all the algorithms use the same training data for all tasks performed in the algorithm, however many of our approaches allow use of different training sets for the learning itself (the \gls{baum-welch}) and determining the states to split -- for instance. Also using methods such as cross-validation could help improve the results and reduce overfitting.

On the final note, more attention can also be directed at the additional methods we have explored in connection to keeping the model sparse and prevent unnecessary overgrowing, such as edge and state removal. Both of those approaches have been explored before yielding successful results~\cite{bicego2003, bicego2007}~and may therefore serve as a foundation or a powerful aid in design of future dynamic learning algorithms despite our decision not to include them in the final versions of tested approaches.