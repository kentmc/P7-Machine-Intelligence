
\subsection{Probabilistic Automatons}
More commonly referred to as simply \gls{pa} or
sometimes Rabin Automaton, like the name of Michael O. Rabin who introduced
this concept in 1963, \gls{pa}s generalise the concept of \gls{nfa}. \gls{nfa}s can transition to zero or more states,
without requiring input symbols for state transitions. However, \gls{pa}s
introduce transitioning probabilities in the definition. Therefore,
whereas \gls{nfa}s all have a Deterministic equivalent, the introduction
of the probability concept in \gls{pa}s prevent the existence of \gls{dfa} equivalents,
unless all probabilities are equal to 0 or 1.

The languages \gls{pa}s recognise are called stochastic languages \cite{macarie} - of which
the regular ones are a subset. The number of stochastic languages is uncountable.
By their definition, \gls{pa}s extend the Markov Chain concept. More formally:

\begin{description}
	\item [{P:}] probability of transitioning to a particular state
	\item [{Q:}] finite set of states
	\item [{$q_{0}$:}] probability vector of being in a particular initial
	state
	\item [{$\varSigma$:}] finite set of input symbols
	\item [{$\delta:Q\times\varSigma\longrightarrow P(Q)$:}] transition function
	\item [{F:}] finite set of final states, with $F\subset Q$
\end{description}

In addition, the transition matrix T from a state q to a state q'
while producing the symbol s must have the following property:

\[
\sum_{q'}[T_{s}]_{qq'}=1
\]


This means that, if a symbol s is produced, the probability of transitioning
from a state to another (or possibly the same) is 1. A Probabilistic
Automaton thus has:
\begin{itemize}
\item A probability of being in a particular initial state
\item A probability of producing a symbol depending on the current state
\item A probability of transitioning from a state to another, depending
on the symbol produced
\item A probability of ending the sequence when in a final state
\end{itemize}
Albeit complex, \gls{pa}s have many applications. Indeed, in most cases,
the only available data is the observed output. It is thus necessary
to learn how to make a model out of these observed sequences to understand
them. \gls{dna} sequences are a good example of this problem. The observed
symbols undoubtedly follow a model of some sort, yet unknown. The
goal of a \gls{pa} is to approximate this kind of model, and learning \gls{pa}s
ultimately leads to learning how \gls{dna} sequences are generated.