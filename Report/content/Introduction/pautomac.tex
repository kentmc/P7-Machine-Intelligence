\section{PautomaC Competition}

\subsection{PautomaC}
PautomaC (Probabilistic Automata learning Competition) was, according to the creators, the first online challenge about learning non-deterministic probabilistic finite state machines.
In the competition, a number of probabilistic automatons have each generated a large set of sequences, which is referred to as a training set.
The contestants does not know anything about these automatons, besides the training set generated by each which has been published.
For each automaton, the contestants should based on the training set try to learn a model of that automaton.
To evaluate a learned model, a test set of 1000 sequences were published. The model should for each sequence provide a probability of generating that sequence. When a contestant submitted a list of normalized probabilities for all sequences, his solution was evaluated by comparing his probabilities to the real probabilities of the model that generated the sequences.
The evaluation where done by using the following perplexity measure:

\begin{equation} \label{eq:perplexity}
2^{-(\sum_{x\epsilon TestSet}P_{r_{T}}(x)\times\log(P_{r_{C}}(x))}
\end{equation}

where $P_{r_{T}}(x)$ represents the real probability of the
sequence $x$, and $P_{r_{C}}$ is a contestants submitted probability for the sequence $x$.

A contestant could submit a solution as many times as he wanted to. He would never get to know his exact score, only his rank among other contestants.
It is worthy to note that the only thing that mattered in the competition was the ability to calculate the right probabilities.
That is, a contestant was free to use any type of model or algorithm for obtaining these probabilities.

\subsubsection{Automatons used}

All the models of the competition were of one the following types:
\begin{enumerate}
\item Markov Chains
\item Probabilistic Finite Automata (PFAs)
\item Deterministic Probabilistic Finite Automata (DPFAs)
\item Hidden Markov Models (HMMs)
\end{enumerate}

All model parameters have been randomly generated based on some values chosen for:
\begin{itemize}
\item N for the number of states
\item A for the size of the alphabet (or number of observed symbols)
\item S for the scarcity of symbols
\item T for the scarcity of transitions\end{itemize}

$NT$ is then used as the number of initial states while $NS$ is used as the number of final states.

\begin{description}
\item [{S}] represents the average percentage of symbols produced by any
given state. In other words, each state will only produce S\% of the
whole alphabet A in average. Do note that two states may still produce
different symbols.
\item [{T}] represents the average percentage of all possible transitions
branching out from any given state. A transition here is a link from
one state to another, with a symbol associated to it. With this in
mind: for each state, out of all the possible transitions branching
out from it (each with their respective symbol and destination state),
only T\% will be generated.
\end{description}



%\subsubsection{The model files}
%
%Generating a test model means that a model file is generated. This
%file contains all the necessary information to generate sequences.
%The information inside those files is displayed in a particular manner
%described as follows:
%\begin{description}
%\item [{I:}] (state number)\\
%Probability of starting the sequence in that state.\\
%\textit{Note: the sum of all those probabilities is equal to 1.}
%\item [{F:}] (state number)\\
%Probably of ending the sequence when in that state\textit{.}\\
%\textit{Note: those probabilities are independent of every other,
%including the symbol and transition ones. Also, their sum is NOT equal
%to 1.}
%\item [{S:}] (state number, symbol number)\\
%Probability of producing this symbol in this state.\\
%\textit{Note: the sum of all the probabilities for any one state is
%equal to 1.}
%\item [{T:}] (starting state number, symbol number, destination state number)\\
%Probability of reaching this destination state from this starting
%state, given this symbol.\\
%\textit{Note: the sum of all the probabilities sharing the same ``starting
%state number'' and ``symbol number'' is equal to 1.}
%\end{description}
%
%\subsubsection{Rating}
%
%At the time of the competition, the model and solution files were,
%of course, not available. The competitors had to provide their solution
%file to each test file. Those solution files did not contain a model,
%though. What they contained instead was, for each sequence of the
%test file, the probability for it to occur. The organisers then compared
%their results to the actual probabilities inside their solution files,
%and rated the competitors based on how close they were. The formula
%they used is:
%
%\[
%2^{-(\sum_{x\epsilon TestSet}P_{r_{T}}(x)\times\log(P_{r_{C}}(x)))}
%\]

\subsubsection{After the competition}
On July 3rd, 2012, the competition was over, and all the secret models were published, as well as the solution probabilities to all the test sequences.
Even though the competition is over, we can still try to learn the models.
That implies of course, that we should not use the data published after the competition ended in our favor.
Thus, we cannot use the probabilities published for the test data for evaluating our algorithms while training them.
The test data should only be used one single time for each algorithm.
However, the training data can be split into two parts, where one of the part is used for training and the other for validation.