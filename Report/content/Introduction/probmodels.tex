\section{Probabilistic automata}
There exist many different automata for learning observations. A general feature of these automatas is that the better an automata can model observations, the harder it will be to train that automata. The best known automatas are the \gls{hmm} and \gls{pfa}, where this report will focus on the use of \gls{hmm}.\cite{pautomacTR}
Other candidates were concidered for this work, including n-gram, Markov chains and some deterministic counterparts to the \gls{pfa}, however all of these are strictly less powerful than the \gls{pfa}. The \gls{hmm} is known to be equivalent with the \gls{pfa}, which makes them equally strong in terms of their modelling power.

Other potential candidates include \gls{pcfg} and \gls{ma}, where the \gls{pcfg} shows strength in bio-informatics and \gls{ma} is known to be stronger than the \gls{pfa}. However as mentioned in section \ref{sec:pautomac} the automatas used to produced the data sets are: \gls{mc}, \gls{dpfa}, \gls{hmm} and \gls{pfa}, where modelling strength beyond \gls{pfa} is not necessary. In practice the \gls{pcfg} would manage the data sets just fine, as the data is finite. However in reality an observed system might emit symbols in one single infinite sequence, where a model based on context free grammar would not be suitable. The reason for this is that the grammatical rules of \gls{pcfg} has to reach a terminal before a string is recognized, which cannot happen on an infinite string.