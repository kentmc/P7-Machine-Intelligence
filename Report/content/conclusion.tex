\chapter{Conclusion}
\label{chap:conclusion}
The report deals with possibilities of data derived inference of \gls{hmm} structure, especially the hidden state graph topology, alongside learning the \gls{hmm} parameters. We offer a deep analysis of the \glspl{hmm} in regards to possibility of using sparse transition matrix for both data derived structure of the \gls{hmm} and the computational  speedup. ased on the analysis we propose several ideas and approaches to the given problem and evaluate them on artificial data generated for the purposes of the PautomaC competition. In general we observe our dynamic algorithms (\acrfull{ge} and \acrfull{gs}) to outperform the the baseline vanilla \gls{baum-welch} or at least produce results in high proximity to that of the \gls{baum-welch}.

We have managed to widely explore the concept of adding states to an \gls{hmm} iteratively to derive the structure of the \gls{hmm} given the data. Several approaches were attempted and we offer thorough explanation of both their strong and weak points and performance. As our results show promise in the use of similar approaches we believe our findings can easily serve as a starting point for further research in the area.

In chapter \ref{chap:models} the two proposed algorithms for learning both the \gls{hmm} parameters and structure are presented, the \acrlong{ge} and \acrlong{gs} algorithms. The \gls{ge} is a rather simple greedy approach to continuously extending the \gls{hmm} by adding new states, while such a state can be found in a reasonable amount of attempts. Th \gls{ge} algorithm expands the \gls{hmm} by including states with logarithmic number of non-zero probability in- and out-transitions thus maintaining a sparse model. This simple approach proved itself to be on par with BW, while being able to dynamically increase the state space. It has also served to prove that an\gls{hmm} with a considerably sparse matrix (less then $2n\log(n)$ transition as opposed to the $n^2$ transitions of a dense matrix) can achieve results good enough to compare or even better than the state-of-the-art \gls{baum-welch}.

