\chapter{Conclusion}
\label{chap:conclusion}
The report deals with possibilities of data derived inference of \gls{hmm} structure, especially the hidden state graph topology, alongside learning the \gls{hmm} parameters. We offer a deep analysis of the \glspl{hmm} in regards to the possibility of using a sparse transition matrix, for both data derived structure of the \gls{hmm} and the computational  speedup. Based on the analysis we propose several ideas and approaches to the given problem and evaluate them on artificial data, generated for the purposes of the PautomaC competition. In general we observe our dynamic algorithms (\acrfull{ge} and \acrfull{gs}) tends to outperform the baseline vanilla \gls{bw} or at least produce results in high proximity to that of the \gls{bw}.

We have managed to explore the concept of adding states to an \gls{hmm} iteratively, to derive a structure for the \gls{hmm} given the data. Several approaches were attempted and we offer an analysis of both their strong and weak points and performance. As our results show promise in the use of similar approaches, we believe our findings can easily serve as a starting point for further research in the area.

In chapter \ref{chap:models} the two proposed algorithms for learning both the parameters and the structure of the \gls{hmm} are presented; the \acrlong{ge} and \acrlong{gs} algorithms. The \gls{ge} is a rather simple greedy approach to continuously extending the \gls{hmm} by adding new random parameter states, while such a state can be found in a reasonable amount of attempts. The \gls{ge} algorithm expands the \gls{hmm} by including states with logarithmic number of non-zero probability in- and out-transitions thus maintaining a sparse model. This simple approach proved itself to be on par with BW, while being able to dynamically increase the state space. It has also served to prove that an\gls{hmm} with a considerably sparse matrix (less then $2n\log(n)$ transition as opposed to the $n^2$ transitions of a dense matrix) can achieve results good enough to compare to the state-of-the-art \gls{bw}.

The \acrlong{gs} algorithm on the other hand does not naively attempt to add a random state, instead a more elaborate greedy heuristic is used for identifying a weak point in the \gls{hmm}, consequently splitting the identified state in attempt to increase performance. We have analysed several approaches that could be used as modules in the state splitting algorithm, to tweak and refine the performance. Many of the combinations of those modules and improvements proposed may require more attention, however we have managed to construct a concise stochastic algorithm for state splitting. Contrary to the initial intention to utilise sparseness of the \gls{hmm} transition matrix, the \gls{gs} runs on a dense matrix. This is mainly due to complexity of fine tuning the parameters for the \gls{gs} algorithm, that still require refining, as such it was more sensible to utilise a basic dense matrix than the more experimental sparse matrix. The \gls{gs} algorithm has achieved almost incomparably better results than the other tested algorithms, we have however also experienced a non-standard behaviour of the algorithm that is being attributed to the underflow problem, however could also indicate an implementation error, which we at this point cannot rule out.

To better illustrate the benefits of the sparse transition matrix we have implemented version of the \gls{bw}, \gls{viterbi} and other \gls{hmm} related algorithms capable of utilising the sparsity of the matrix to decrease the computational complexity. We offer a speed comparison of the newly implemented algorithms against the standard version computing on a dense matrix. It was not possible to achieve the theoretical speedup on the most critical \gls{bw}, however it should be noted, that our optimisation is not perfect and further improvement is possible, that could further increase the speedup compared to standard methods.

A multitude of experiments has been done on four algorithms: \acrlong{ge}, \acrlong{gs}, \acrlong{bw}, \acrlong{sbw}. Three types of experiments were done in total. The first of the experiments was conducted on several different datasets, available at the PautomaC website. During those experiments we have experienced extremely varied behaviour of all the algorithms on different datasets. We have concentrated on three available parameters of the models that generated the PautomaC datasets, and despite the fact that several conclusions and hypotheses have been drawn from the results we were unable to find a clear patterns that enforced the different behaviour of individual algorithms on different datasets.

The results obtained from the dataset experiment show large promise for our approaches as in several cases the \gls{ge} and \gls{gs} algorithms outperformed the \gls{baum-welch}.

Another concluded experiment is a speed comparison between \acrlong{ge}, \acrlong{bw} and \acrlong{sbw} algorithms. The algorithms were left to run for 8 hours to see how many results can be obtained during the given time scope. We have made two very important observations during those test that may, however, be biased by the relatively narrow scope of the experiment. First being an extreme domination of the \gls{ge} algorithm over the \gls{bw} and \gls{sbw} algorithms in both the score learned and the number of states achieved in the given time scope. Second result was a very unstable behaviour of the \gls{sbw} approach, suggesting that a random sparse transition matrix induces a lot of non-deterministic behaviour in the algorithm and that a data derived approach to building the sparse transition matrix of an \gls{hmm} might be required instead.

Last of the experiments was a direct comparison with the PautomaC scores. Here we have observed the tested algorithms to perform unsatisfactory. This is mostly attributed to the use of a different paradigm when it comes to probability distribution over the strings, as generated by the model. Whilst PautomaC used stopping probability to induce distribution over all signals of all lengths, we have not used any stopping probability in our algorithms thus generating probability distribution over sequences of each length separately, introducing less probability into the computation of the PautomaC evaluation criterion. In consequence we disqualified ourselves from the capability of achieving scores comparable to the contestants.

In general we can claim to have done in depth analysis of the problem at hand and proposed several solutions that are conceivably adequate, in the scope of the problem definition. We have to admit that much more work can be done, which we suggest in the following chapter.