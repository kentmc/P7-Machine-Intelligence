\chapter{Conclusion}

In chapter \ref{chap:model} two algorithms are presented for solving the problem definition; the Greed Extend (GE) and Gamma Splitter(GS) algorithms. The \gls{ge} is a very naive approach that randomly maintain a Hidden Markov Model (HMM) with at most $log(n)$ transitions, where $n$ is the amount of states. \gls{ge} run 5 iterations of Baum-Welch (BW), thereafter it adds a random state with random parameters to the model, maintaining at most $log(n)$ transitions. This simple approach proved itself to be on par with BW, while being able to dynamically increase the state space. The Gamme splitter (GE) algorithm works not by adding states with random parameters, but by splitting the parameters from an existing state between the existing state and a newly added state. This approach produced likelihoods comparable to \gls{bw} on a couple of Pautomac competition datasets, but on most it produced significantly higher likelihoods. Unfortunately some of the datasets caused the \gls{gs} to underflow on large state spaces, which calls for a more aggressive scaling scheme.
The \gls{gs} algorithm overfitting behaviour showed itself to behave unusual
