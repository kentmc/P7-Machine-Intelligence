\chapter{Conclusion}
\label{chap:conclusion}
The report deals with possibilities of data derived inference of \gls{hmm} structure, especially the hidden state graph topology, alongside learning the \gls{hmm} parameters. We offer a deep analysis of the \glspl{hmm} in regards to possibility of using sparse transition matrix for both data derived structure of the \gls{hmm} and the computational  speedup. Based on the analysis we propose several ideas and approaches to the given problem and evaluate them on artificial data generated for the purposes of the \emph{PAutomaC} competition. In general we observe our dynamic algorithms (\acrfull{ge} and \acrfull{gs}) tends to outperform the baseline vanilla \gls{baum-welch} or at least produce results in high proximity to that of the \gls{baum-welch}.

We have managed to widely explore the concept of adding states to an \gls{hmm} iteratively to derive the structure of the \gls{hmm} given the data. Several approaches were attempted and we offer thorough explanation of both their strong and weak points and performance. As our results show promise in the use of similar approaches we believe our findings can easily serve as a starting point for further research in the area.

In chapter \ref{chap:models} the two proposed algorithms for learning both the \gls{hmm} parameters and structure are presented, the \acrlong{ge} and \acrlong{gs} algorithms. The \gls{ge} is a rather simple greedy approach to continuously extending the \gls{hmm} by adding new states, while such a state can be found in a reasonable amount of attempts. Th \gls{ge} algorithm expands the \gls{hmm} by including states with logarithmic number of non-zero probability in- and out-transitions thus maintaining a sparse model. This simple approach proved itself to be on par with BW, while being able to dynamically increase the state space. It has also served to prove that an\gls{hmm} with a considerably sparse matrix (less then $2n\log(n)$ transition as opposed to the $n^2$ transitions of a dense matrix) can achieve results good enough to compare or even better than the state-of-the-art \gls{baum-welch}.

The \acrlong{gs} algorithm on the other hand does not naively attempt adding a random state and utilises a more elaborate greedy heuristic for identifying a weak point in the \gls{hmm} and consequently splitting the identified state in attempt to increase performance. We have analysed several approaches that could be used as modules in the state splitting algorithm to tweak and refine the performance. Many of the combinations of those modules and improvements proposed may require more attention, however we have managed to construct a concise stochastic algorithm for state splitting, titled \acrlong{gs}. Contrary to the initial intention to utilise sparseness of the \gls{hmm} transition matrix, the \gls{gs} runs on a dense matrix. This is mainly due to complexity of fine tuning the parameters for the \gls{gs} algorithm that may still require refining, as such it was more sensible to utilise a standard well known dense matrix than the more experimental sparse matrix. The \gls{gs} algorithm has often achieved almost incomparably better results than the other tested algorithms, we have however also experienced a non-standard behaviour of the algorithm that is being attributed to the underflow problem, however could also indicate an implementation bug, which we at this point cannot rule out.

To better illustrate the benefits of the sparse transition matrix we have implemented an optimised version of the \gls{baum-welch}, \gls{viterbi} and other \gls{hmm} related algorithms capable of utilising the sparsity of the matrix to decrease the computational complexity. We offer a speed comparison of the newly implemented algorithms against the standard version computing on a dense matrix. We have not been capable of achieving the theoretical speedup on the most critical \gls{baum-welch}, however it should be noted, that our optimisation is not perfect and further improvement is possible that could further increase the speedup compared to standard methods.

We have conducted a multitude of experiments on four algorithms: \acrlong{ge}, \acrlong{gs}, \acrlong{bw}, \acrlong{sbw}. Three types of experiments were done in total. The first of the experiments was conducted on several different datasets available on the \emph{PAutomaC} website. During those experiments we have experienced extremely varied behaviour of all the algorithms on different datasets. We have concentrated on three most relevant parameters of the models that generated the \emph{PAutomaC} datasets used in the tests and despite the fact that several conclusions and hypotheses have been drawn from the results we were unable to find the exact patterns that enforced the vastly different behaviour of individual algorithms on different datasets.

The results obtained from the dataset experiment show large promise for our approaches as in several cases the \gls{ge} and \gls{gs} algorithms outperformed the \gls{baum-welch}.

Another concluded experiment is a speed comparison between \acrlong{ge}, \acrlong{bw} and \acrlong{sbw} algorithms. The algorithms were left to run for 8 hours to see how many results can be obtained during the given time scope. We have made two very important observations during those test that may, however, be biased by the relatively narrow scope of the experiment. First being an extreme domination of the \gls{ge} algorithm over the \gls{bw} and \gls{sbw} algorithms in both the score learned and the number of states achieved in the given time scope. Second result was a very unstable behaviour of the \gls{sbw} approach suggesting a random sparse transition matrix induces a lot of noise in the algorithm and a data derived approach to building the sparse transition matrix of an \gls{hmm} might be required instead.

Last of the experiments was a direct comparison with the \emph{PAutomaC} scores. Here we have observed the tested algorithms to perform unsatisfactory. This is mostly attributed to the usage of different paradigm when it comes to probability distribution over the signal as define by the model. Whilst \emph{PAutomaC} used stopping probability to induce distribution over all signals of all lengths we have not used any stopping probability in our algorithms thus generating probability distribution over sequences of each length separately introducing noise into the computation of the \emph{PAutomaC} evaluation criterion. In consequence we disqualified ourselves from the capability of achieving scores comparable to the contestants.

In general we can claim to have done in depth analysis of the problem at hand and proposed several solutions that are conceivably adequate in the environment we work with. On the other hand we have to admit that much more work can be done in the area, which we suggest in the following chapter.