\chapter{Conclusion}
\label{chap:conclusion}
The report deals with possibilities of data derived inference of \gls{hmm} structure, especially the hidden state graph topology, alongside learning the \gls{hmm} parameters. We offer a deep analysis of the \glspl{hmm} in regards to possibility of using sparse transition matrix for both data derived structure of the \gls{hmm} and the computational  speedup. Based on the analysis we propose several ideas and approaches to the given problem and evaluate them on artificial data generated for the purposes of the PautomaC competition. In general we observe our dynamic algorithms (\acrfull{ge} and \acrfull{gs}) to outperform the the baseline vanilla \gls{baum-welch} or at least produce results in high proximity to that of the \gls{baum-welch}.

We have managed to widely explore the concept of adding states to an \gls{hmm} iteratively to derive the structure of the \gls{hmm} given the data. Several approaches were attempted and we offer thorough explanation of both their strong and weak points and performance. As our results show promise in the use of similar approaches we believe our findings can easily serve as a starting point for further research in the area.

In chapter \ref{chap:models} the two proposed algorithms for learning both the \gls{hmm} parameters and structure are presented, the \acrlong{ge} and \acrlong{gs} algorithms. The \gls{ge} is a rather simple greedy approach to continuously extending the \gls{hmm} by adding new states, while such a state can be found in a reasonable amount of attempts. Th \gls{ge} algorithm expands the \gls{hmm} by including states with logarithmic number of non-zero probability in- and out-transitions thus maintaining a sparse model. This simple approach proved itself to be on par with BW, while being able to dynamically increase the state space. It has also served to prove that an\gls{hmm} with a considerably sparse matrix (less then $2n\log(n)$ transition as opposed to the $n^2$ transitions of a dense matrix) can achieve results good enough to compare or even better than the state-of-the-art \gls{baum-welch}.

The \acrlong{gs} algorithm on the other hand does not naively attempt adding a random state and utilises a more elaborate greedy heuristic for identifying a weak point in the \gls{hmm} and consequently splitting the identified state in attempt to increase performance. We have analysed several approaches that could be used as modules in the state splitting algorithm to tweak and refine the performance. Many of the combinations of those modules and improvements proposed may require more attention, however we have managed to construct a concise stochastic algorithm for state splitting, titled \acrlong{gs}. Contrary to the initial intention to utilise sparseness of the \gls{hmm} transition matrix, the \gls{gs} runs on a dense matrix. This is mainly due to complexity of fine tuning the parameters for the \gls{gs} algorithm that may still require refining, as such it was more sensible to utilise a standard well known dense matrix than the more experimental sparse matrix. The \gls{gs} algorithm has often achieved almost incomparably better results than the other tested algorithms, we have however also experienced a non-standard behaviour of the algorithm that is being attributed to the underflow problem, however could also indicate an implementation bug, which we at this point cannot rule out.



