\chapter{Conclusion}
\label{chap:conclusion}
The report deals with possibilities of data derived inference of \gls{hmm} structure, especially the hidden state graph topology, alongside learning the \gls{hmm} parameters. We have managed to offer several ideas and approaches to the given problem and evaluate them on artificial data generated for the purposes of the PautomaC competition. In general we observe our algorithms (\acrfull{ge} and \acrfull{gs}) to outperform the the baseline vanilla \gls{baum-welch} or at least produce results in high proximity to that of the \gls{baum-welch}.

In our report we offer a deep analysis of the \glspl{hmm} in regards to possibility of using sparse transition matrix for both data derived structure of the \gls{hmm} and the computational speedup.

In chapter \ref{chap:models} two algorithms are presented for solving the problem definition; the \acrfull{ge} and \acrfull{gs} algorithms. The \gls{ge} is a very naive approach that randomly maintain a Hidden Markov Model (HMM) with at most $log(n)$ transitions, where $n$ is the amount of states. \gls{ge} run 5 iterations of Baum-Welch (BW), thereafter it adds a random state with random parameters to the model, maintaining at most $log(n)$ transitions. This simple approach proved itself to be on par with BW, while being able to dynamically increase the state space. The Gamme splitter (GE) algorithm works not by adding states with random parameters, but by splitting the parameters from an existing state between the existing state and a newly added state. This approach produced likelihoods comparable to \gls{bw} on a couple of Pautomac competition datasets, but on most it produced significantly higher likelihoods. Unfortunately some of the datasets caused the \gls{gs} to underflow on large state spaces, which calls for a more aggressive scaling scheme.
The \gls{gs} algorithm overfitting behaviour showed itself to behave unusual
